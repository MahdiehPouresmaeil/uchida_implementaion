{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": ""
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ST98LX_z8uHh"
   },
   "source": [
    "# **Base model with embedding watermark**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zcbTIZqIF7zS"
   },
   "source": [
    "\n",
    "Epoch [1/10], Test Accuracy: 64.96%, Classification Loss: 1.7705, Watermark Loss: 0.0070\n",
    "\n",
    "Original Watermark: [0. 1. 0. 1. 1. 0. 0. 1.]\n",
    "\n",
    "Extracted Watermark: [1. 1. 0. 0. 0. 1. 1. 0.]\n",
    "\n",
    "BER 0.75\n",
    "Epoch [2/10], Test Accuracy: 97.11%, Classification Loss: 1.5053, Watermark Loss: 0.0069\n",
    "\n",
    "Original Watermark: [0. 1. 0. 1. 1. 0. 0. 1.]\n",
    "\n",
    "Extracted Watermark: [0. 1. 0. 0. 0. 0. 1. 0.]\n",
    "\n",
    "BER 0.5\n",
    "Epoch [3/10], Test Accuracy: 97.67%, Classification Loss: 1.4900, Watermark Loss: 0.0068\n",
    "\n",
    "Original Watermark: [0. 1. 0. 1. 1. 0. 0. 1.]\n",
    "\n",
    "Extracted Watermark: [0. 1. 0. 0. 1. 0. 1. 0.]\n",
    "\n",
    "BER 0.375\n",
    "Epoch [4/10], Test Accuracy: 98.05%, Classification Loss: 1.4667, Watermark Loss: 0.0068\n",
    "\n",
    "Original Watermark: [0. 1. 0. 1. 1. 0. 0. 1.]\n",
    "\n",
    "Extracted Watermark: [0. 1. 0. 0. 1. 0. 1. 0.]\n",
    "\n",
    "BER 0.375\n",
    "Epoch [5/10], Test Accuracy: 98.00%, Classification Loss: 1.4613, Watermark Loss: 0.0067\n",
    "\n",
    "Original Watermark: [0. 1. 0. 1. 1. 0. 0. 1.]\n",
    "\n",
    "Extracted Watermark: [0. 1. 0. 0. 1. 0. 1. 0.]\n",
    "\n",
    "BER 0.375\n",
    "Epoch [6/10], Test Accuracy: 98.17%, Classification Loss: 1.4720, Watermark Loss: 0.0066\n",
    "\n",
    "Original Watermark: [0. 1. 0. 1. 1. 0. 0. 1.]\n",
    "\n",
    "Extracted Watermark: [0. 1. 0. 1. 1. 0. 1. 0.]\n",
    "\n",
    "BER 0.25\n",
    "Epoch [7/10], Test Accuracy: 98.36%, Classification Loss: 1.4615, Watermark Loss: 0.0066\n",
    "\n",
    "Original Watermark: [0. 1. 0. 1. 1. 0. 0. 1.]\n",
    "\n",
    "Extracted Watermark: [0. 1. 0. 1. 1. 0. 0. 1.]\n",
    "\n",
    "BER 0.0\n",
    "✅ Model saved with Test Accuracy: 98.36% and BER: 0.0000 and bestepoch: 7\n",
    "Epoch [8/10], Test Accuracy: 98.38%, Classification Loss: 1.4703, Watermark Loss: 0.0065\n",
    "\n",
    "Original Watermark: [0. 1. 0. 1. 1. 0. 0. 1.]\n",
    "\n",
    "Extracted Watermark: [0. 1. 0. 1. 1. 0. 0. 1.]\n",
    "\n",
    "BER 0.0\n",
    "✅ Model saved with Test Accuracy: 98.38% and BER: 0.0000 and bestepoch: 8\n",
    "Epoch [9/10], Test Accuracy: 98.46%, Classification Loss: 1.4671, Watermark Loss: 0.0064\n",
    "\n",
    "Original Watermark: [0. 1. 0. 1. 1. 0. 0. 1.]\n",
    "\n",
    "Extracted Watermark: [0. 1. 0. 1. 1. 0. 0. 1.]\n",
    "\n",
    "BER 0.0\n",
    "✅ Model saved with Test Accuracy: 98.46% and BER: 0.0000 and bestepoch: 9\n",
    "Epoch [10/10], Test Accuracy: 98.42%, Classification Loss: 1.4612, Watermark Loss: 0.0064\n",
    "\n",
    "Original Watermark: [0. 1. 0. 1. 1. 0. 0. 1.]\n",
    "\n",
    "Extracted Watermark: [0. 1. 0. 1. 1. 0. 0. 1.]\n",
    "\n",
    "BER 0.0\n",
    "✅Final  Model Test Accuracy: 98.46% and BER: 0.0000 and bestepoch: 9\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import torch.optim.lr_scheduler as lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-03-28T13:07:24.966081Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "NocXpnQb4Ych",
    "jupyter": {
     "is_executing": true,
     "outputs_hidden": true
    },
    "outputId": "adf0de91-755b-43f0-f206-ea10d867569f"
   },
   "outputs": [],
   "source": [
    "#USHIDA PAPER\n",
    "#when LR 1e-3 BER stock in 0.125 when LR 1e-2 accuracy model is not good\n",
    "\n",
    "#BASE_EMBEDDED_MODEL\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# !rm -rf ./data/MNIST\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "seed = 58  # You can choose any number\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)  # If using multiple GPUs\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.backends.cudnn.deterministic = True  # Ensure deterministic behavior\n",
    "torch.backends.cudnn.benchmark = False  # Disable benchmark for reproducibility\n",
    "\n",
    "\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Hyperparameters\n",
    "num_epochs =10\n",
    "\n",
    "batch_size = 256\n",
    "learning_rate =0.001\n",
    "lambda_wm =1e-2 # 1e-2 Watermark regularizer 0.01\n",
    "best_acc = 0.0  # Track best test accuracy\n",
    "best_ber = float(\"inf\")  # Track lowest BER\n",
    "best_model_path = \"best_model.pth\"\n",
    "\n",
    "\n",
    "# Load dataset\n",
    "transform = transforms.Compose([\n",
    "    # transforms.RandomRotation(10),  # Rotate images by ±10 degrees\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, transform=transform, download=True)  # Test set\n",
    "\n",
    "# Use the same seed for the DataLoader shuffle\n",
    "g = torch.Generator()\n",
    "g.manual_seed(seed)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, worker_init_fn=lambda _: np.random.seed(seed), generator=g)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "# Computes the Bit Error Rate (BER)\n",
    "def compute_ber(original_watermark, extracted_watermark):\n",
    "  diff = original_watermark - extracted_watermark\n",
    "  num_errors = torch.sum(torch.abs(diff))\n",
    "  ber = num_errors.float() / original_watermark.numel()\n",
    "  return ber.item() #float\n",
    "\n",
    "\n",
    "\n",
    "# Define Watermark Regularizer\n",
    "class WatermarkRegularizer(nn.Module):\n",
    "    def __init__(self, lambda_wm, watermark_vector, C_in, K):\n",
    "        super(WatermarkRegularizer, self).__init__()\n",
    "        self.lambda_wm = lambda_wm  # Watermark regularizer\n",
    "        self.watermark_vector = watermark_vector  #  watermark vector\n",
    "        T=watermark_vector.shape[0]\n",
    "        M = C_in*K*K  # Hidden dimension for projection\n",
    "        self.secret_key = torch.randn(T, M, device=device)\n",
    "        # self.secret_key = torch.nn.functional.normalize(self.secret_key, p=2, dim=1)\n",
    "        # print(self.secret_key.shape)   #8,9\n",
    "\n",
    "    def forward(self, weights):\n",
    "        # print(weights.size())   #32,1,3,3\n",
    "        w_mean = weights.mean(dim=(0))  # mean of filters\n",
    "        # print(w_mean.size())   #1*3*3\n",
    "        w_mean_flat = w_mean.view(-1)  # Flatten(C_in * K * K)\n",
    "        # print(w_mean_flat.size())   #9\n",
    "        # print(self.secret_key.T.size())  #9,8\n",
    "        projected_wm = torch.sigmoid(torch.matmul (self.secret_key,w_mean_flat))  # Compute WX\n",
    "        # wm_loss=self.lambda_wm * torch.norm(projected_wm - self.watermark_vector)  # Regularization loss\n",
    "        wm_loss = self.lambda_wm * nn.BCELoss(reduction='mean')(projected_wm.to(device), self.watermark_vector.to(device))\n",
    "        # print((projected_wm > 0.5).float())\n",
    "        # print(self.watermark_vector)\n",
    "        # print('***************************************')\n",
    "\n",
    "        return wm_loss\n",
    "\n",
    "# Define a Simple CNN with Watermark Embedding\n",
    "class WatermarkedCNN(nn.Module):\n",
    "    def __init__(self, watermark_vector):\n",
    "        super(WatermarkedCNN, self).__init__()\n",
    "        # self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        # self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(32 * 28 *28, 512)\n",
    "        self.fc2 = nn.Linear(512, 10)\n",
    "        # self.wm_regularizer = WatermarkRegularizer(lambda_wm, watermark_vector, C_in=1, K=3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv2(x)) # watermark is here\n",
    "        # print(x.shape)\n",
    "        # wm_loss = self.wm_regularizer(self.conv2.weight)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # print(x.shape)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        # x=torch.sigmoid(x)\n",
    "        x = torch.softmax(x, dim=1)\n",
    "        return x#, wm_loss\n",
    "\n",
    "# Initialize model\n",
    "# generate a random watermark, ignoring the seed\n",
    "# rand_gen = torch.Generator(device)  # Create a new generator\n",
    "# rand_gen.seed()  # Seed it randomly\n",
    "\n",
    "# # Generate a new random watermark vector using this independent generator\n",
    "# watermark_vector = torch.randint(0, 2, (256,), dtype=torch.float32, device=device, generator=rand_gen)\n",
    "\n",
    "\n",
    "# watermark_vector = torch.randint(0, 2, (256,), dtype=torch.float32, device=device)  # Binary watermark\n",
    "\n",
    "\n",
    "\n",
    "watermark_vector=torch.tensor([0., 1., 0., 1., 1., 0., 0., 1.], dtype=torch.float32)\n",
    "\n",
    "# print(watermark_vector)\n",
    "# print(watermark_vector.size()) #8\n",
    "\n",
    "\n",
    "model = WatermarkedCNN(watermark_vector).to(device)\n",
    "wm_regularizer = WatermarkRegularizer(lambda_wm, watermark_vector, C_in=1, K=3)\n",
    "torch.save(wm_regularizer.secret_key, \"secret_key.pth\")  # Save the secret key\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "# Training the model\n",
    "# import numpy as np\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss_class = criterion(outputs, labels)\n",
    "        wm_loss = wm_regularizer(model.conv2.weight)\n",
    "\n",
    "        loss = loss_class + wm_loss  # Total loss\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "    # current_lr = optimizer.param_groups[0]['lr']\n",
    "    # print(f\"Epoch {epoch+1}/{num_epochs}, Learning Rate: {current_lr:.6f}\")\n",
    "    # scheduler.step()\n",
    "\n",
    "\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():  # No gradients needed\n",
    "          for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)  # Get class index with highest probability\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()  # Count correct predictions\n",
    "\n",
    "    accuracy = 100 * correct / total  # Compute accuracy percentage\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Test Accuracy: {accuracy:.2f}%, Classification Loss: {loss_class.item():.4f}, \"\n",
    "          f\"Watermark Loss: {wm_loss.item():.4f}\")\n",
    "    # print(f\"Epoch [{epoch+1}/{num_epochs}],  Classification Loss: {loss_class.item():.4f}, \"\n",
    "    #       f\"Watermark Loss: {wm_loss.item():.4f}\")\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "      conv2_mean = model.conv2.weight.mean(dim=0)  # Should be (1,3,3)\n",
    "      conv2_mean_flat = conv2_mean.view(-1)\n",
    "      extracted_watermark = torch.sigmoid(torch.matmul( wm_regularizer.secret_key.cpu(), conv2_mean_flat.cpu()))  # Fix dimensions\n",
    "      extracted_watermark_binary = (extracted_watermark > 0.5).float()\n",
    "\n",
    "      # print(type(extracted_watermark_binary))\n",
    "      # print(type(watermark_vector))\n",
    "\n",
    "      print(\"\\nOriginal Watermark:\", watermark_vector.cpu().numpy())\n",
    "      print(\"\\nExtracted Watermark:\", extracted_watermark_binary.cpu().numpy())\n",
    "      print(\"\\nBER\", compute_ber(watermark_vector.cpu(), extracted_watermark_binary.cpu()))\n",
    "\n",
    "\n",
    "    ber=compute_ber(watermark_vector.cpu(), extracted_watermark_binary.cpu())\n",
    "    if accuracy>best_acc and ber<=0.0:\n",
    "        if os.path.exists(best_model_path):\n",
    "          os.remove(best_model_path)\n",
    "        best_acc=accuracy\n",
    "        best_ber=ber\n",
    "        best_epoch=epoch+1\n",
    "        best_model_path = f\"best_model_lR:{learning_rate}_lamda:{lambda_wm}_Acc:{best_acc}_Epoch:{best_epoch}_Ber:{best_ber}.pth\"\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        print(f\"✅ Model saved with Test Accuracy: {best_acc:.2f}% and BER: {best_ber:.4f} and bestepoch: {best_epoch}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Extract the watermark after training\n",
    "# with torch.no_grad():\n",
    "#     conv2_mean = model.conv2.weight.mean(dim=0)  # Should be (1,3,3)\n",
    "#     conv2_mean_flat = conv2_mean.view(-1)\n",
    "#     extracted_watermark = torch.sigmoid(torch.matmul( wm_regularizer.secret_key, conv2_mean_flat))  # Fix dimensions\n",
    "#     extracted_watermark_binary = (extracted_watermark > 0.5).float()\n",
    "\n",
    "\n",
    "# Display results\n",
    "# print(\"\\nOriginal Watermark:\", watermark_vector.cpu().numpy())\n",
    "# print(\"\\nExtracted Watermark:\", extracted_watermark_binary.cpu().numpy())\n",
    "\n",
    "print(f\"✅Final  Model Test Accuracy: {best_acc:.2f}% and BER: {best_ber:.4f} and bestepoch: {best_epoch}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "874PmbC4CRbq"
   },
   "source": [
    "# **Base model without watermark**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "weecT2AyogzN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "EbKW3MdVCRLG",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "67e12ebe-2b4d-4676-c8b0-051b7b158c67"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9.91M/9.91M [00:00<00:00, 15.8MB/s]\n",
      "100%|██████████| 28.9k/28.9k [00:00<00:00, 478kB/s]\n",
      "100%|██████████| 1.65M/1.65M [00:00<00:00, 4.45MB/s]\n",
      "100%|██████████| 4.54k/4.54k [00:00<00:00, 7.98MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Test Accuracy: 58.31%, Classification Loss: 1.8655\n",
      "✅ Model saved with Test Accuracy: 58.31%  and bestepoch: 1\n",
      "Epoch [2/10], Test Accuracy: 77.43%, Classification Loss: 1.7098\n",
      "✅ Model saved with Test Accuracy: 77.43%  and bestepoch: 2\n",
      "Epoch [3/10], Test Accuracy: 86.63%, Classification Loss: 1.5568\n",
      "✅ Model saved with Test Accuracy: 86.63%  and bestepoch: 3\n",
      "Epoch [4/10], Test Accuracy: 96.28%, Classification Loss: 1.4715\n",
      "✅ Model saved with Test Accuracy: 96.28%  and bestepoch: 4\n",
      "Epoch [5/10], Test Accuracy: 97.99%, Classification Loss: 1.4927\n",
      "✅ Model saved with Test Accuracy: 97.99%  and bestepoch: 5\n",
      "Epoch [6/10], Test Accuracy: 98.11%, Classification Loss: 1.5033\n",
      "✅ Model saved with Test Accuracy: 98.11%  and bestepoch: 6\n",
      "Epoch [7/10], Test Accuracy: 98.03%, Classification Loss: 1.4632\n",
      "Epoch [8/10], Test Accuracy: 98.29%, Classification Loss: 1.4612\n",
      "✅ Model saved with Test Accuracy: 98.29%  and bestepoch: 8\n",
      "Epoch [9/10], Test Accuracy: 98.36%, Classification Loss: 1.4816\n",
      "✅ Model saved with Test Accuracy: 98.36%  and bestepoch: 9\n",
      "Epoch [10/10], Test Accuracy: 98.17%, Classification Loss: 1.4716\n",
      "✅Final  Model Test Accuracy: 98.36% and bestepoch: 9\n"
     ]
    }
   ],
   "source": [
    "#USHIDA PAPER\n",
    "#when LR 1e-3 BER stock in 0.125 when LR 1e-2 accuracy model is not good\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "\n",
    "# !rm -rf ./data/MNIST\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Set random seed for reproducibility\n",
    "# seed = 42  # You can choose any number\n",
    "# torch.manual_seed(seed)\n",
    "# torch.cuda.manual_seed_all(seed)  # If using multiple GPUs\n",
    "# np.random.seed(seed)\n",
    "# random.seed(seed)\n",
    "# torch.backends.cudnn.deterministic = True  # Ensure deterministic behavior\n",
    "# torch.backends.cudnn.benchmark = False  # Disable benchmark for reproducibility\n",
    "\n",
    "\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Hyperparameters\n",
    "num_epochs =10\n",
    "\n",
    "batch_size = 256\n",
    "learning_rate =0.001\n",
    "best_acc = 0.0  # Track best test accuracy\n",
    "best_model_path = \"best_model2.pth\"\n",
    "\n",
    "\n",
    "# Load dataset\n",
    "transform = transforms.Compose([\n",
    "    # transforms.RandomRotation(10),  # Rotate images by ±10 degrees\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, transform=transform, download=True)  # Test set\n",
    "\n",
    "# Use the same seed for the DataLoader shuffle\n",
    "# g = torch.Generator()\n",
    "# g.manual_seed(seed)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)#, worker_init_fn=lambda _: np.random.seed(seed), generator=g)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "# Define a Simple CNN with Watermark Embedding\n",
    "class WatermarkedCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(WatermarkedCNN, self).__init__()\n",
    "        # self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        # self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(32 * 28 *28, 512)\n",
    "        self.fc2 = nn.Linear(512, 10)\n",
    "        # self.wm_regularizer = WatermarkRegularizer(lambda_wm, watermark_vector, C_in=1, K=3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv2(x)) # watermark is here\n",
    "        # print(x.shape)\n",
    "        # wm_loss = self.wm_regularizer(self.conv2.weight)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # print(x.shape)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        # x=torch.sigmoid(x)\n",
    "        x = torch.softmax(x, dim=1)\n",
    "        return x#, wm_loss\n",
    "\n",
    "\n",
    "\n",
    "model = WatermarkedCNN().to(device)\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss_class = criterion(outputs, labels)\n",
    "        loss = loss_class  # Total loss\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():  # No gradients needed\n",
    "          for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)  # Get class index with highest probability\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()  # Count correct predictions\n",
    "\n",
    "    accuracy = 100 * correct / total  # Compute accuracy percentage\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Test Accuracy: {accuracy:.2f}%, Classification Loss: {loss_class.item():.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "    if accuracy>best_acc :\n",
    "        if os.path.exists(best_model_path):\n",
    "          os.remove(best_model_path)\n",
    "        best_acc=accuracy\n",
    "        best_epoch=epoch+1\n",
    "        best_model_path = f\"best_model_lR:{learning_rate}_Acc:{best_acc}_Epoch:{best_epoch}.pth\"\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        print(f\"✅ Model saved with Test Accuracy: {best_acc:.2f}%  and bestepoch: {best_epoch}\")\n",
    "\n",
    "\n",
    "print(f\"✅Final  Model Test Accuracy: {best_acc:.2f}% and bestepoch: {best_epoch}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TygIUrqq8j64"
   },
   "source": [
    "# **base:embeded model fine tuning without embeding- fine tune all parametres**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CQcmQHwyGgMy"
   },
   "source": [
    "Original Watermark: [0. 1. 0. 1. 1. 0. 0. 1.]\n",
    "\n",
    "Extracted Watermark: [0. 1. 0. 1. 1. 0. 0. 1.]\n",
    "\n",
    "BER 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VTrqJljt5LXn"
   },
   "outputs": [],
   "source": [
    "# Assuming you have saved your model as 'model.pth'\n",
    "LR=1e-3\n",
    "model_finetune = WatermarkedCNN().to(device)  # Assuming WatermarkedCNN is your model class\n",
    "model_finetune.load_state_dict(torch.load('/content/BASE_MODEL_EMBED_best_model_lR_0.001_lamda_0.01_Acc_98.46_Epoch_9_Ber_0.0.pth'))\n",
    "model_finetune.fc2 = nn.Linear(512, 10).to(device)\n",
    "model_finetune.train()\n",
    "optimizer = optim.Adam(model_finetune.parameters(), lr=LR)  #fine tune all parameteres\n",
    "# optimizer = optim.Adam(model_finetune.fc2.parameters(), lr=1e-3) #finetune just last layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jMq-ny7b72aZ",
    "outputId": "3fd532ee-d54b-4f33-b5fc-673761978f6a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Test Accuracy: 88.87%, Classification Loss: 1.5818\n",
      "✅ Model saved with Test Accuracy: 88.87% and bestepoch: 1\n",
      "Epoch [2/10], Test Accuracy: 88.95%, Classification Loss: 1.5814\n",
      "✅ Model saved with Test Accuracy: 88.95% and bestepoch: 2\n",
      "Epoch [3/10], Test Accuracy: 89.00%, Classification Loss: 1.5121\n",
      "✅ Model saved with Test Accuracy: 89.00% and bestepoch: 3\n",
      "Epoch [4/10], Test Accuracy: 98.49%, Classification Loss: 1.4621\n",
      "✅ Model saved with Test Accuracy: 98.49% and bestepoch: 4\n",
      "Epoch [5/10], Test Accuracy: 98.52%, Classification Loss: 1.4612\n",
      "✅ Model saved with Test Accuracy: 98.52% and bestepoch: 5\n",
      "Epoch [6/10], Test Accuracy: 98.48%, Classification Loss: 1.4618\n",
      "Epoch [7/10], Test Accuracy: 98.50%, Classification Loss: 1.4615\n",
      "Epoch [8/10], Test Accuracy: 98.38%, Classification Loss: 1.4622\n",
      "Epoch [9/10], Test Accuracy: 98.52%, Classification Loss: 1.4612\n",
      "Epoch [10/10], Test Accuracy: 98.49%, Classification Loss: 1.4613\n"
     ]
    }
   ],
   "source": [
    "num_finetune_epochs = 10  # Adjust as needed\n",
    "best_finetune_acc = 0.0  # Track best test accuracy\n",
    "best_model_path = \"best_model_finetune.pth\"\n",
    "for epoch in range(num_finetune_epochs):\n",
    "    model_finetune.train()  # Ensure model is in training mode\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model_finetune(images)\n",
    "        loss = criterion(outputs, labels)  # Compute loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "    model_finetune.eval()  # Set model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():  # No gradients needed\n",
    "          for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs =model_finetune(images)\n",
    "            _, predicted = torch.max(outputs, 1)  # Get class index with highest probability\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()  # Count correct predictions\n",
    "\n",
    "    accuracy = 100 * correct / total  # Compute accuracy percentage\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_finetune_epochs}], Test Accuracy: {accuracy:.2f}%, Classification Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # Save the fine-tuned model\n",
    "    if accuracy>best_finetune_acc :\n",
    "        if os.path.exists(best_model_path):\n",
    "          os.remove(best_model_path)\n",
    "        best_finetune_acc =accuracy\n",
    "        best_epoch=epoch+1\n",
    "        best_model_path = f\"best_finetuned_model_lR:{LR}_Acc:{best_finetune_acc}_Epoch:{best_epoch}.pth\"\n",
    "        torch.save(model_finetune.state_dict(), best_model_path)\n",
    "        print(f\"✅ Model saved with Test Accuracy: {best_finetune_acc:.2f}% and bestepoch: {best_epoch}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 245
    },
    "collapsed": true,
    "id": "ZGgrhEnK937q",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "c6931eed-5980-49ac-ca9d-909a9e42bc04"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Watermark: [0. 1. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Extracted Watermark: [0. 1. 1. 1. 0. 0. 0. 1.]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'compute_ber' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-13-4e91bc0d359d>\u001B[0m in \u001B[0;36m<cell line: 0>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     40\u001B[0m       \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"\\nOriginal Watermark:\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mwatermark_vector\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcpu\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnumpy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     41\u001B[0m       \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"\\nExtracted Watermark:\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mextracted_watermark_binary\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcpu\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnumpy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 42\u001B[0;31m       \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"\\nBER\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcompute_ber\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mwatermark_vector\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcpu\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mextracted_watermark_binary\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcpu\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m: name 'compute_ber' is not defined"
     ]
    }
   ],
   "source": [
    "wm_regularizer.secret_key = torch.load(\"secret_key.pth\", map_location=device)\n",
    "with torch.no_grad():\n",
    "      conv2_mean = model_finetune.conv2.weight.mean(dim=0)  # Should be (1,3,3)\n",
    "      conv2_mean_flat = conv2_mean.view(-1)\n",
    "      extracted_watermark = torch.sigmoid(torch.matmul( wm_regularizer.secret_key.cpu(), conv2_mean_flat.cpu()))  # Fix dimensions\n",
    "      extracted_watermark_binary = (extracted_watermark > 0.5).float()\n",
    "\n",
    "      # print(type(extracted_watermark_binary))\n",
    "      # print(type(watermark_vector))\n",
    "\n",
    "      print(\"\\nOriginal Watermark:\", watermark_vector.cpu().numpy())\n",
    "      print(\"\\nExtracted Watermark:\", extracted_watermark_binary.cpu().numpy())\n",
    "      print(\"\\nBER\", compute_ber(watermark_vector.cpu(), extracted_watermark_binary.cpu()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VVNnuZRHmeD"
   },
   "source": [
    "#**base:NOT embeded model fine tuning without embeding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XKLfYOZCHoKS"
   },
   "outputs": [],
   "source": [
    "LR=1e-3\n",
    "model_finetune = WatermarkedCNN(watermark_vector).to(device)  # Assuming WatermarkedCNN is your model class\n",
    "model_finetune.load_state_dict(torch.load('/content/best_model_WITHHOUT_watermark_lR:0.001_Acc:98.51_Epoch:8.pth'))\n",
    "model_finetune.fc2 = nn.Linear(512, 10).to(device)\n",
    "model_finetune.train()\n",
    "optimizer = optim.Adam(model_finetune.parameters(), lr=LR)  #fine tune all parameteres\n",
    "# optimizer = optim.Adam(model_finetune.fc2.parameters(), lr=LR) #finetune just last layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GVayqGy7H_Hl",
    "outputId": "6ccfe99f-d253-494b-e89f-e918e67549f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Test Accuracy: 98.00%, Classification Loss: 1.4699\n",
      "✅ Model saved with Test Accuracy: 98.00% and bestepoch: 1\n",
      "Epoch [2/10], Test Accuracy: 98.00%, Classification Loss: 1.4821\n",
      "Epoch [3/10], Test Accuracy: 98.00%, Classification Loss: 1.4812\n",
      "Epoch [4/10], Test Accuracy: 98.00%, Classification Loss: 1.4663\n",
      "Epoch [5/10], Test Accuracy: 98.00%, Classification Loss: 1.4617\n",
      "Epoch [6/10], Test Accuracy: 98.00%, Classification Loss: 1.4612\n",
      "Epoch [7/10], Test Accuracy: 98.00%, Classification Loss: 1.4619\n",
      "Epoch [8/10], Test Accuracy: 98.00%, Classification Loss: 1.4695\n",
      "Epoch [9/10], Test Accuracy: 98.00%, Classification Loss: 1.4612\n",
      "Epoch [10/10], Test Accuracy: 98.00%, Classification Loss: 1.4716\n"
     ]
    }
   ],
   "source": [
    "num_finetune_epochs = 10  # Adjust as needed\n",
    "best_finetune_acc = 0.0  # Track best test accuracy\n",
    "best_model_path = \"best_model_finetune.pth\"\n",
    "for epoch in range(num_finetune_epochs):\n",
    "    model_finetune.train()  # Ensure model is in training mode\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model_finetune(images)\n",
    "        loss = criterion(outputs, labels)  # Compute loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "    model_finetune.eval()  # Set model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():  # No gradients needed\n",
    "          for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model_finetune(images)\n",
    "            _, predicted = torch.max(outputs, 1)  # Get class index with highest probability\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()  # Count correct predictions\n",
    "\n",
    "    accuracy = 100 * correct / total  # Compute accuracy percentage\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_finetune_epochs}], Test Accuracy: {accuracy:.2f}%, Classification Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # Save the fine-tuned model\n",
    "    if accuracy>best_finetune_acc :\n",
    "        if os.path.exists(best_model_path):\n",
    "          os.remove(best_model_path)\n",
    "        best_finetune_acc =accuracy\n",
    "        best_epoch=epoch+1\n",
    "        best_model_path = f\"best_finetuned_model_lR:{LR}_Acc:{best_finetune_acc}_Epoch:{best_epoch}.pth\"\n",
    "        torch.save(model_finetune.state_dict(), best_model_path)\n",
    "        print(f\"✅ Model saved with Test Accuracy: {best_finetune_acc:.2f}% and bestepoch: {best_epoch}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PN8fNFCxIBdE",
    "outputId": "222a3898-7f88-4037-b981-3e2cb0fd3cb2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Watermark: [0. 1. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Extracted Watermark: [0. 1. 1. 0. 1. 0. 0. 0.]\n",
      "\n",
      "BER 0.375\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "      conv2_mean = model_finetune.conv2.weight.mean(dim=0)  # Should be (1,3,3)\n",
    "      conv2_mean_flat = conv2_mean.view(-1)\n",
    "      extracted_watermark = torch.sigmoid(torch.matmul( wm_regularizer.secret_key.cpu(), conv2_mean_flat.cpu()))  # Fix dimensions\n",
    "      extracted_watermark_binary = (extracted_watermark > 0.5).float()\n",
    "\n",
    "      # print(type(extracted_watermark_binary))\n",
    "      # print(type(watermark_vector))\n",
    "\n",
    "      print(\"\\nOriginal Watermark:\", watermark_vector.cpu().numpy())\n",
    "      print(\"\\nExtracted Watermark:\", extracted_watermark_binary.cpu().numpy())\n",
    "      print(\"\\nBER\", compute_ber(watermark_vector.cpu(), extracted_watermark_binary.cpu()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JyTNPP90MxsL"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AvjhGtvlMy-e"
   },
   "source": [
    "# **base embeded--- model fine tune with same embeding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I7KnNygbM1Ct"
   },
   "outputs": [],
   "source": [
    "LR=1e-3\n",
    "model_finetune = WatermarkedCNN().to(device)  # Assuming WatermarkedCNN is your model class\n",
    "model_finetune.load_state_dict(torch.load('/content/BASE_best_model_WITH_watermark_lR:0.001_lamda:0.01_Acc:98.46_Epoch:9_Ber:0.0.pth'))\n",
    "model_finetune.fc2 = nn.Linear(512, 10).to(device)\n",
    "model_finetune.train()\n",
    "optimizer = optim.Adam(model_finetune.parameters(), lr=LR)  #fine tune all parameteres\n",
    "# optimizer = optim.Adam(model_finetune.fc2.parameters(), lr=1e-3) #finetune just last layer\n",
    "\n",
    "num_finetune_epochs = 10  # Adjust as needed\n",
    "best_finetune_acc = 0.0  # Track best test accuracy\n",
    "best_model_path = \"best_model_finetune.pth\"\n",
    "batch_size = 256\n",
    "lambda_wm =1e-2 # 1e-2 Watermark regularizer 0.01\n",
    "\n",
    "best_ber = float(\"inf\")  # Track lowest BER\n",
    "\n",
    "\n",
    "\n",
    "watermark_vector=torch.tensor([0., 1., 0., 1., 1., 0., 0., 1.], dtype=torch.float32)\n",
    "\n",
    "# print(watermark_vector)\n",
    "# print(watermark_vector.size()) #8\n",
    "\n",
    "\n",
    "\n",
    "wm_regularizer = WatermarkRegularizer(lambda_wm, watermark_vector, C_in=1, K=3)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "# Training the model\n",
    "# import numpy as np\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model_finetune.train()\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model_finetune(images)\n",
    "        loss_class = criterion(outputs, labels)\n",
    "        wm_loss = wm_regularizer(model_finetune.conv2.weight)\n",
    "\n",
    "        loss = loss_class + wm_loss  # Total loss\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "    # current_lr = optimizer.param_groups[0]['lr']\n",
    "    # print(f\"Epoch {epoch+1}/{num_epochs}, Learning Rate: {current_lr:.6f}\")\n",
    "    # scheduler.step()\n",
    "\n",
    "\n",
    "    model_finetune.eval()  # Set model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():  # No gradients needed\n",
    "          for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model_finetune(images)\n",
    "            _, predicted = torch.max(outputs, 1)  # Get class index with highest probability\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()  # Count correct predictions\n",
    "\n",
    "    accuracy = 100 * correct / total  # Compute accuracy percentage\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Test Accuracy: {accuracy:.2f}%, Classification Loss: {loss_class.item():.4f}, \"\n",
    "          f\"Watermark Loss: {wm_loss.item():.4f}\")\n",
    "    # print(f\"Epoch [{epoch+1}/{num_epochs}],  Classification Loss: {loss_class.item():.4f}, \"\n",
    "    #       f\"Watermark Loss: {wm_loss.item():.4f}\")\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "      conv2_mean = model_finetune.conv2.weight.mean(dim=0)  # Should be (1,3,3)\n",
    "      conv2_mean_flat = conv2_mean.view(-1)\n",
    "      extracted_watermark = torch.sigmoid(torch.matmul( wm_regularizer.secret_key.cpu(), conv2_mean_flat.cpu()))  # Fix dimensions\n",
    "      extracted_watermark_binary = (extracted_watermark > 0.5).float()\n",
    "\n",
    "      # print(type(extracted_watermark_binary))\n",
    "      # print(type(watermark_vector))\n",
    "\n",
    "      print(\"\\nOriginal Watermark:\", watermark_vector.cpu().numpy())\n",
    "      print(\"\\nExtracted Watermark:\", extracted_watermark_binary.cpu().numpy())\n",
    "      print(\"\\nBER\", compute_ber(watermark_vector.cpu(), extracted_watermark_binary.cpu()))\n",
    "\n",
    "\n",
    "    ber=compute_ber(watermark_vector.cpu(), extracted_watermark_binary.cpu())\n",
    "    if accuracy>best_finetune_acc and ber<=best_ber:\n",
    "        if os.path.exists(best_model_path):\n",
    "          os.remove(best_model_path)\n",
    "        best_acc=accuracy\n",
    "        best_ber=ber\n",
    "        best_epoch=epoch+1\n",
    "        best_model_path = f\"best_model_lR:{learning_rate}_lamda:{lambda_wm}_Acc:{best_finetune_acc}_Epoch:{best_epoch}_Ber:{best_ber}.pth\"\n",
    "        torch.save(model_finetune.state_dict(), best_model_path)\n",
    "        print(f\"✅ Model saved with Test Accuracy: {best_acc:.2f}% and BER: {best_ber:.4f} and bestepoch: {best_epoch}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Extract the watermark after training\n",
    "# with torch.no_grad():\n",
    "#     conv2_mean = model.conv2.weight.mean(dim=0)  # Should be (1,3,3)\n",
    "#     conv2_mean_flat = conv2_mean.view(-1)\n",
    "#     extracted_watermark = torch.sigmoid(torch.matmul( wm_regularizer.secret_key, conv2_mean_flat))  # Fix dimensions\n",
    "#     extracted_watermark_binary = (extracted_watermark > 0.5).float()\n",
    "\n",
    "\n",
    "# Display results\n",
    "# print(\"\\nOriginal Watermark:\", watermark_vector.cpu().numpy())\n",
    "# print(\"\\nExtracted Watermark:\", extracted_watermark_binary.cpu().numpy())\n",
    "\n",
    "print(f\"✅Final  Model Test Accuracy: {best_finetune_acc:.2f}% and BER: {best_ber:.4f} and bestepoch: {best_epoch}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O47etvCsNEYt"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "874PmbC4CRbq"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
