{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": ""
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ST98LX_z8uHh"
   },
   "source": [
    "# **Base model with embedding watermark**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zcbTIZqIF7zS"
   },
   "source": [
    "\n",
    "Epoch [1/10], Test Accuracy: 64.96%, Classification Loss: 1.7705, Watermark Loss: 0.0070\n",
    "\n",
    "Original Watermark: [0. 1. 0. 1. 1. 0. 0. 1.]\n",
    "\n",
    "Extracted Watermark: [1. 1. 0. 0. 0. 1. 1. 0.]\n",
    "\n",
    "BER 0.75\n",
    "Epoch [2/10], Test Accuracy: 97.11%, Classification Loss: 1.5053, Watermark Loss: 0.0069\n",
    "\n",
    "Original Watermark: [0. 1. 0. 1. 1. 0. 0. 1.]\n",
    "\n",
    "Extracted Watermark: [0. 1. 0. 0. 0. 0. 1. 0.]\n",
    "\n",
    "BER 0.5\n",
    "Epoch [3/10], Test Accuracy: 97.67%, Classification Loss: 1.4900, Watermark Loss: 0.0068\n",
    "\n",
    "Original Watermark: [0. 1. 0. 1. 1. 0. 0. 1.]\n",
    "\n",
    "Extracted Watermark: [0. 1. 0. 0. 1. 0. 1. 0.]\n",
    "\n",
    "BER 0.375\n",
    "Epoch [4/10], Test Accuracy: 98.05%, Classification Loss: 1.4667, Watermark Loss: 0.0068\n",
    "\n",
    "Original Watermark: [0. 1. 0. 1. 1. 0. 0. 1.]\n",
    "\n",
    "Extracted Watermark: [0. 1. 0. 0. 1. 0. 1. 0.]\n",
    "\n",
    "BER 0.375\n",
    "Epoch [5/10], Test Accuracy: 98.00%, Classification Loss: 1.4613, Watermark Loss: 0.0067\n",
    "\n",
    "Original Watermark: [0. 1. 0. 1. 1. 0. 0. 1.]\n",
    "\n",
    "Extracted Watermark: [0. 1. 0. 0. 1. 0. 1. 0.]\n",
    "\n",
    "BER 0.375\n",
    "Epoch [6/10], Test Accuracy: 98.17%, Classification Loss: 1.4720, Watermark Loss: 0.0066\n",
    "\n",
    "Original Watermark: [0. 1. 0. 1. 1. 0. 0. 1.]\n",
    "\n",
    "Extracted Watermark: [0. 1. 0. 1. 1. 0. 1. 0.]\n",
    "\n",
    "BER 0.25\n",
    "Epoch [7/10], Test Accuracy: 98.36%, Classification Loss: 1.4615, Watermark Loss: 0.0066\n",
    "\n",
    "Original Watermark: [0. 1. 0. 1. 1. 0. 0. 1.]\n",
    "\n",
    "Extracted Watermark: [0. 1. 0. 1. 1. 0. 0. 1.]\n",
    "\n",
    "BER 0.0\n",
    "✅ Model saved with Test Accuracy: 98.36% and BER: 0.0000 and bestepoch: 7\n",
    "Epoch [8/10], Test Accuracy: 98.38%, Classification Loss: 1.4703, Watermark Loss: 0.0065\n",
    "\n",
    "Original Watermark: [0. 1. 0. 1. 1. 0. 0. 1.]\n",
    "\n",
    "Extracted Watermark: [0. 1. 0. 1. 1. 0. 0. 1.]\n",
    "\n",
    "BER 0.0\n",
    "✅ Model saved with Test Accuracy: 98.38% and BER: 0.0000 and bestepoch: 8\n",
    "Epoch [9/10], Test Accuracy: 98.46%, Classification Loss: 1.4671, Watermark Loss: 0.0064\n",
    "\n",
    "Original Watermark: [0. 1. 0. 1. 1. 0. 0. 1.]\n",
    "\n",
    "Extracted Watermark: [0. 1. 0. 1. 1. 0. 0. 1.]\n",
    "\n",
    "BER 0.0\n",
    "✅ Model saved with Test Accuracy: 98.46% and BER: 0.0000 and bestepoch: 9\n",
    "Epoch [10/10], Test Accuracy: 98.42%, Classification Loss: 1.4612, Watermark Loss: 0.0064\n",
    "\n",
    "Original Watermark: [0. 1. 0. 1. 1. 0. 0. 1.]\n",
    "\n",
    "Extracted Watermark: [0. 1. 0. 1. 1. 0. 0. 1.]\n",
    "\n",
    "BER 0.0\n",
    "✅Final  Model Test Accuracy: 98.46% and BER: 0.0000 and bestepoch: 9\n",
    "\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T13:55:54.498545Z",
     "start_time": "2025-03-28T13:55:54.491992Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import torch.optim.lr_scheduler as lr_scheduler"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T14:23:13.669562Z",
     "start_time": "2025-03-28T14:23:13.661367Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Set random seed for reproducibility\n",
    "seed = 58  # You can choose any number\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)  # If using multiple GPUs\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.backends.cudnn.deterministic = True  # Ensure deterministic behavior\n",
    "torch.backends.cudnn.benchmark = False  # Disable benchmark for reproducibility\n"
   ],
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T13:55:58.358443Z",
     "start_time": "2025-03-28T13:55:58.353192Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T13:55:59.441566Z",
     "start_time": "2025-03-28T13:55:59.434218Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Hyperparameters\n",
    "num_epochs =10\n",
    "batch_size = 256\n",
    "learning_rate =0.001\n",
    "lambda_wm =1e-2 # 1e-2 Watermark regularizer 0.01\n",
    "best_acc = 0.0  # Track best test accuracy\n",
    "best_ber = float(\"inf\")  # Track lowest BER\n",
    "best_model_path = \"best_model.pth\"\n"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T13:56:01.150446Z",
     "start_time": "2025-03-28T13:56:01.015058Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load dataset\n",
    "\n",
    "\n",
    "# !rm -rf ./data/MNIST\n",
    "\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    # transforms.RandomRotation(10),  # Rotate images by ±10 degrees\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, transform=transform, download=True)  # Test set\n",
    "\n",
    "# Use the same seed for the DataLoader shuffle\n",
    "g = torch.Generator()\n",
    "g.manual_seed(seed)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, worker_init_fn=lambda _: np.random.seed(seed), generator=g)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T13:56:03.198939Z",
     "start_time": "2025-03-28T13:56:03.192047Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Computes the Bit Error Rate (BER)\n",
    "def compute_ber(original_watermark, extracted_watermark):\n",
    "  diff = original_watermark - extracted_watermark\n",
    "  num_errors = torch.sum(torch.abs(diff))\n",
    "  ber = num_errors.float() / original_watermark.numel()\n",
    "  return ber.item() #float\n"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T13:56:04.806494Z",
     "start_time": "2025-03-28T13:56:04.799290Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define Watermark Regularizer\n",
    "class WatermarkRegularizer(nn.Module):\n",
    "    def __init__(self, lambda_wm, watermark_vector, C_in, K):\n",
    "        super(WatermarkRegularizer, self).__init__()\n",
    "        self.lambda_wm = lambda_wm  # Watermark regularizer\n",
    "        self.watermark_vector = watermark_vector  #  watermark vector\n",
    "        T=watermark_vector.shape[0]\n",
    "        M = C_in*K*K  # Hidden dimension for projection\n",
    "        self.secret_key = torch.randn(T, M, device=device)\n",
    "        # self.secret_key = torch.nn.functional.normalize(self.secret_key, p=2, dim=1)\n",
    "        # print(self.secret_key.shape)   #8,9\n",
    "\n",
    "    def forward(self, weights):\n",
    "        # print(weights.size())   #32,1,3,3\n",
    "        w_mean = weights.mean(dim=(0))  # mean of filters\n",
    "        # print(w_mean.size())   #1*3*3\n",
    "        w_mean_flat = w_mean.view(-1)  # Flatten(C_in * K * K)\n",
    "        # print(w_mean_flat.size())   #9\n",
    "        # print(self.secret_key.T.size())  #9,8\n",
    "        projected_wm = torch.sigmoid(torch.matmul (self.secret_key,w_mean_flat))  # Compute WX\n",
    "        # wm_loss=self.lambda_wm * torch.norm(projected_wm - self.watermark_vector)  # Regularization loss\n",
    "        wm_loss = self.lambda_wm * nn.BCELoss(reduction='mean')(projected_wm.to(device), self.watermark_vector.to(device))\n",
    "        # print((projected_wm > 0.5).float())\n",
    "        # print(self.watermark_vector)\n",
    "        # print('***************************************')\n",
    "\n",
    "        return wm_loss"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T13:56:21.054375Z",
     "start_time": "2025-03-28T13:56:21.043606Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define a Simple CNN with Watermark Embedding\n",
    "class WatermarkedCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(WatermarkedCNN, self).__init__()\n",
    "        # self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        # self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(32 * 28 *28, 512)\n",
    "        self.fc2 = nn.Linear(512, 10)\n",
    "        # self.wm_regularizer = WatermarkRegularizer(lambda_wm, watermark_vector, C_in=1, K=3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv2(x)) # watermark is here\n",
    "        # print(x.shape)\n",
    "        # wm_loss = self.wm_regularizer(self.conv2.weight)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # print(x.shape)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        # x=torch.sigmoid(x)\n",
    "        x = torch.softmax(x, dim=1)\n",
    "        return x#, wm_loss\n"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "NocXpnQb4Ych",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "adf0de91-755b-43f0-f206-ea10d867569f",
    "ExecuteTime": {
     "end_time": "2025-03-28T13:58:55.101816Z",
     "start_time": "2025-03-28T13:56:25.746038Z"
    }
   },
   "source": [
    "\n",
    "#BASE_EMBEDDED_MODEL\n",
    "\n",
    "# Initialize model\n",
    "# generate a random watermark, ignoring the seed\n",
    "# rand_gen = torch.Generator(device)  # Create a new generator\n",
    "# rand_gen.seed()  # Seed it randomly\n",
    "\n",
    "# # Generate a new random watermark vector using this independent generator\n",
    "# watermark_vector = torch.randint(0, 2, (256,), dtype=torch.float32, device=device, generator=rand_gen)\n",
    "\n",
    "\n",
    "# watermark_vector = torch.randint(0, 2, (256,), dtype=torch.float32, device=device)  # Binary watermark\n",
    "\n",
    "\n",
    "\n",
    "watermark_vector=torch.tensor([0., 1., 0., 1., 1., 0., 0., 1.], dtype=torch.float32)\n",
    "\n",
    "# print(watermark_vector)\n",
    "# print(watermark_vector.size()) #8\n",
    "\n",
    "\n",
    "model = WatermarkedCNN().to(device)\n",
    "wm_regularizer = WatermarkRegularizer(lambda_wm, watermark_vector, C_in=1, K=3)\n",
    "torch.save(wm_regularizer.secret_key, \"secret_key.pth\")  # Save the secret key\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "# Training the model\n",
    "# import numpy as np\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss_class = criterion(outputs, labels)\n",
    "        wm_loss = wm_regularizer(model.conv2.weight)\n",
    "        loss = loss_class + wm_loss  # Total loss\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # current_lr = optimizer.param_groups[0]['lr']\n",
    "    # print(f\"Epoch {epoch+1}/{num_epochs}, Learning Rate: {current_lr:.6f}\")\n",
    "    # scheduler.step()\n",
    "\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():  # No gradients needed\n",
    "          for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)  # Get class index with highest probability\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()  # Count correct predictions\n",
    "    accuracy = 100 * correct / total  # Compute accuracy percentage\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Test Accuracy: {accuracy:.2f}%, Classification Loss: {loss_class.item():.4f}, \"\n",
    "          f\"Watermark Loss: {wm_loss.item():.4f}\")\n",
    "    # print(f\"Epoch [{epoch+1}/{num_epochs}],  Classification Loss: {loss_class.item():.4f}, \"\n",
    "    #       f\"Watermark Loss: {wm_loss.item():.4f}\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "      conv2_mean = model.conv2.weight.mean(dim=0)  # Should be (1,3,3)\n",
    "      conv2_mean_flat = conv2_mean.view(-1)\n",
    "      extracted_watermark = torch.sigmoid(torch.matmul( wm_regularizer.secret_key.cpu(), conv2_mean_flat.cpu()))  # Fix dimensions\n",
    "      extracted_watermark_binary = (extracted_watermark > 0.5).float()\n",
    "      # print(type(extracted_watermark_binary))\n",
    "      # print(type(watermark_vector))\n",
    "      print(\"\\nOriginal Watermark:\", watermark_vector.cpu().numpy())\n",
    "      print(\"\\nExtracted Watermark:\", extracted_watermark_binary.cpu().numpy())\n",
    "      print(\"\\nBER\", compute_ber(watermark_vector.cpu(), extracted_watermark_binary.cpu()))\n",
    "\n",
    "    ber=compute_ber(watermark_vector.cpu(), extracted_watermark_binary.cpu())\n",
    "    if accuracy>best_acc and ber<=0.0:\n",
    "        if os.path.exists(best_model_path):\n",
    "          os.remove(best_model_path)\n",
    "        best_acc=accuracy\n",
    "        best_ber=ber\n",
    "        best_epoch=epoch+1\n",
    "        best_model_path = f\"best_model_lR:{learning_rate}_lamda:{lambda_wm}_Acc:{best_acc}_Epoch:{best_epoch}_Ber:{best_ber}.pth\"\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        print(f\"✅ Model saved with Test Accuracy: {best_acc:.2f}% and BER: {best_ber:.4f} and bestepoch: {best_epoch}\")\n",
    "\n",
    "print(f\"✅Final  Model Test Accuracy: {best_acc:.2f}% and BER: {best_ber:.4f} and bestepoch: {best_epoch}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Test Accuracy: 86.15%, Classification Loss: 1.5913, Watermark Loss: 0.0068\n",
      "\n",
      "Original Watermark: [0. 1. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Extracted Watermark: [0. 1. 1. 1. 1. 1. 1. 0.]\n",
      "\n",
      "BER 0.5\n",
      "Epoch [2/10], Test Accuracy: 87.35%, Classification Loss: 1.6720, Watermark Loss: 0.0066\n",
      "\n",
      "Original Watermark: [0. 1. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Extracted Watermark: [0. 1. 1. 1. 1. 1. 0. 1.]\n",
      "\n",
      "BER 0.25\n",
      "Epoch [3/10], Test Accuracy: 88.01%, Classification Loss: 1.5680, Watermark Loss: 0.0065\n",
      "\n",
      "Original Watermark: [0. 1. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Extracted Watermark: [0. 1. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "BER 0.0\n",
      "✅ Model saved with Test Accuracy: 88.01% and BER: 0.0000 and bestepoch: 3\n",
      "Epoch [4/10], Test Accuracy: 97.98%, Classification Loss: 1.4762, Watermark Loss: 0.0064\n",
      "\n",
      "Original Watermark: [0. 1. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Extracted Watermark: [0. 1. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "BER 0.0\n",
      "✅ Model saved with Test Accuracy: 97.98% and BER: 0.0000 and bestepoch: 4\n",
      "Epoch [5/10], Test Accuracy: 98.09%, Classification Loss: 1.4861, Watermark Loss: 0.0063\n",
      "\n",
      "Original Watermark: [0. 1. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Extracted Watermark: [0. 1. 0. 0. 1. 0. 0. 1.]\n",
      "\n",
      "BER 0.125\n",
      "Epoch [6/10], Test Accuracy: 98.48%, Classification Loss: 1.4753, Watermark Loss: 0.0062\n",
      "\n",
      "Original Watermark: [0. 1. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Extracted Watermark: [0. 1. 0. 0. 1. 0. 0. 1.]\n",
      "\n",
      "BER 0.125\n",
      "Epoch [7/10], Test Accuracy: 98.31%, Classification Loss: 1.4621, Watermark Loss: 0.0061\n",
      "\n",
      "Original Watermark: [0. 1. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Extracted Watermark: [0. 1. 0. 0. 1. 0. 0. 1.]\n",
      "\n",
      "BER 0.125\n",
      "Epoch [8/10], Test Accuracy: 98.37%, Classification Loss: 1.4725, Watermark Loss: 0.0060\n",
      "\n",
      "Original Watermark: [0. 1. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Extracted Watermark: [0. 1. 0. 0. 1. 0. 0. 1.]\n",
      "\n",
      "BER 0.125\n",
      "Epoch [9/10], Test Accuracy: 98.30%, Classification Loss: 1.4715, Watermark Loss: 0.0059\n",
      "\n",
      "Original Watermark: [0. 1. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Extracted Watermark: [0. 1. 0. 0. 1. 0. 0. 1.]\n",
      "\n",
      "BER 0.125\n",
      "Epoch [10/10], Test Accuracy: 98.52%, Classification Loss: 1.4613, Watermark Loss: 0.0058\n",
      "\n",
      "Original Watermark: [0. 1. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Extracted Watermark: [0. 1. 0. 0. 1. 0. 0. 1.]\n",
      "\n",
      "BER 0.125\n",
      "✅Final  Model Test Accuracy: 97.98% and BER: 0.0000 and bestepoch: 4\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "874PmbC4CRbq"
   },
   "source": [
    "# **Base model without watermark**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "weecT2AyogzN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "EbKW3MdVCRLG",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "67e12ebe-2b4d-4676-c8b0-051b7b158c67",
    "ExecuteTime": {
     "end_time": "2025-03-28T14:25:45.813804Z",
     "start_time": "2025-03-28T14:23:20.779860Z"
    }
   },
   "source": [
    "#BASE_MODEL_NOT_EMBEDED\n",
    "# # generate a random watermark, ignoring the seed\n",
    "# rand_gen = torch.Generator(device)  # Create a new generator\n",
    "# rand_gen.seed()  # Seed it randomly\n",
    "best_model_path = \"best_model2.pth\"\n",
    "\n",
    "\n",
    "model = WatermarkedCNN().to(device)\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss_class = criterion(outputs, labels)\n",
    "        loss = loss_class  # Total loss\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():  # No gradients needed\n",
    "          for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)  # Get class index with highest probability\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()  # Count correct predictions\n",
    "\n",
    "    accuracy = 100 * correct / total  # Compute accuracy percentage\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Test Accuracy: {accuracy:.2f}%, Classification Loss: {loss_class.item():.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "    if accuracy>best_acc :\n",
    "        if os.path.exists(best_model_path):\n",
    "          os.remove(best_model_path)\n",
    "        best_acc=accuracy\n",
    "        best_epoch=epoch+1\n",
    "        best_model_path = f\"best_model_lR:{learning_rate}_Acc:{best_acc}_Epoch:{best_epoch}.pth\"\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        print(f\"✅ Model saved with Test Accuracy: {best_acc:.2f}%  and bestepoch: {best_epoch}\")\n",
    "\n",
    "\n",
    "print(f\"✅Final  Model Test Accuracy: {best_acc:.2f}% and bestepoch: {best_epoch}\")\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Test Accuracy: 86.19%, Classification Loss: 1.5423\n",
      "Epoch [2/10], Test Accuracy: 87.02%, Classification Loss: 1.5721\n",
      "Epoch [3/10], Test Accuracy: 88.05%, Classification Loss: 1.5643\n",
      "Epoch [4/10], Test Accuracy: 97.81%, Classification Loss: 1.4625\n",
      "Epoch [5/10], Test Accuracy: 97.85%, Classification Loss: 1.4852\n",
      "Epoch [6/10], Test Accuracy: 98.49%, Classification Loss: 1.4727\n",
      "✅ Model saved with Test Accuracy: 98.49%  and bestepoch: 6\n",
      "Epoch [7/10], Test Accuracy: 98.46%, Classification Loss: 1.4615\n",
      "Epoch [8/10], Test Accuracy: 98.21%, Classification Loss: 1.4623\n",
      "Epoch [9/10], Test Accuracy: 98.52%, Classification Loss: 1.4716\n",
      "✅ Model saved with Test Accuracy: 98.52%  and bestepoch: 9\n",
      "Epoch [10/10], Test Accuracy: 98.19%, Classification Loss: 1.4877\n",
      "✅Final  Model Test Accuracy: 98.52% and bestepoch: 9\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TygIUrqq8j64"
   },
   "source": [
    "# **base:embeded model fine tuning without embeding- fine tune all parametres**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CQcmQHwyGgMy"
   },
   "source": [
    "Original Watermark: [0. 1. 0. 1. 1. 0. 0. 1.]\n",
    "\n",
    "Extracted Watermark: [0. 1. 0. 1. 1. 0. 0. 1.]\n",
    "\n",
    "BER 0.0"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "VTrqJljt5LXn",
    "ExecuteTime": {
     "end_time": "2025-03-28T15:18:35.007151Z",
     "start_time": "2025-03-28T15:18:34.771008Z"
    }
   },
   "source": [
    "#fine tunning with the dataSET to see watermark still exist or not\n",
    "rand_gen = torch.Generator(device)  # Create a new generator\n",
    "rand_gen.seed()  # Seed it randomly\n",
    "# Assuming you have saved your model as 'model.pth'\n",
    "LR=0.0001\n",
    "model_finetune = WatermarkedCNN().to(device)  # Assuming WatermarkedCNN is your model class\n",
    "model_finetune.load_state_dict(torch.load('BASE-EMBEDDED_MODEL_lR:0.001_lamda:0.01_Acc:97.98_Epoch:4_Ber:0.0.pth'))\n",
    "# model_finetune.fc2 = nn.Linear(512, 10).to(device)\n",
    "model_finetune.train()\n",
    "\n",
    "\n",
    "# for name, param in model_finetune.named_parameters():\n",
    "#     print(f\"{name}: requires_grad={param.requires_grad}\")\n",
    "\n",
    "optimizer = optim.Adam(model_finetune.parameters(), lr=LR)  #fine tune all parameteres\n",
    "# optimizer = optim.Adam(model_finetune.fc2.parameters(), lr=1e-3) #finetune just last layer\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 53
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jMq-ny7b72aZ",
    "outputId": "3fd532ee-d54b-4f33-b5fc-673761978f6a",
    "ExecuteTime": {
     "end_time": "2025-03-28T15:21:01.365562Z",
     "start_time": "2025-03-28T15:18:37.343314Z"
    }
   },
   "source": [
    "\n",
    "num_finetune_epochs = 10  # Adjust as needed\n",
    "best_finetune_acc = 0.0  # Track best test accuracy\n",
    "best_model_path = \"best_model_finetune.pth\"\n",
    "for epoch in range(num_finetune_epochs):\n",
    "    model_finetune.train()  # Ensure model is in training mode\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model_finetune(images)\n",
    "\n",
    "        loss = criterion(outputs, labels)  # Compute loss\n",
    "        loss.backward()\n",
    "\n",
    "        # for name, param in model_finetune.named_parameters():\n",
    "        #   if param.requires_grad and param.grad is not None:\n",
    "        #       print(f\"{name} - Grad Norm: {param.grad.norm().item()}\")\n",
    "        #\n",
    "        #\n",
    "        # for name, param in model_finetune.named_parameters():\n",
    "        #    if param.requires_grad:\n",
    "        #       print(f\"{name} - Grad After Backward: {param.grad}\")\n",
    "\n",
    "        # for param_group in optimizer.param_groups:\n",
    "        #     print(param_group['lr'])\n",
    "\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        # print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
    "\n",
    "\n",
    "\n",
    "    model_finetune.eval()  # Set model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():  # No gradients needed\n",
    "          for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs =model_finetune(images)\n",
    "            _, predicted = torch.max(outputs, 1)  # Get class index with highest probability\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()  # Count correct predictions\n",
    "\n",
    "    accuracy = 100 * correct / total  # Compute accuracy percentage\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_finetune_epochs}], Test Accuracy: {accuracy:.2f}%, Classification Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # Save the fine-tuned model\n",
    "    if accuracy>best_finetune_acc :\n",
    "        if os.path.exists(best_model_path):\n",
    "          os.remove(best_model_path)\n",
    "        best_finetune_acc =accuracy\n",
    "        best_epoch=epoch+1\n",
    "        best_model_path = f\"best_finetuned_model_lR:{LR}_Acc:{best_finetune_acc}_Epoch:{best_epoch}.pth\"\n",
    "        torch.save(model_finetune.state_dict(), best_model_path)\n",
    "        print(f\"✅ Model saved with Test Accuracy: {best_finetune_acc:.2f}% and bestepoch: {best_epoch}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Test Accuracy: 98.46%, Classification Loss: 1.4728\n",
      "✅ Model saved with Test Accuracy: 98.46% and bestepoch: 1\n",
      "Epoch [2/10], Test Accuracy: 98.41%, Classification Loss: 1.4717\n",
      "Epoch [3/10], Test Accuracy: 98.51%, Classification Loss: 1.4618\n",
      "✅ Model saved with Test Accuracy: 98.51% and bestepoch: 3\n",
      "Epoch [4/10], Test Accuracy: 98.50%, Classification Loss: 1.4626\n",
      "Epoch [5/10], Test Accuracy: 98.60%, Classification Loss: 1.4616\n",
      "✅ Model saved with Test Accuracy: 98.60% and bestepoch: 5\n",
      "Epoch [6/10], Test Accuracy: 98.59%, Classification Loss: 1.4616\n",
      "Epoch [7/10], Test Accuracy: 98.52%, Classification Loss: 1.4819\n",
      "Epoch [8/10], Test Accuracy: 98.67%, Classification Loss: 1.4615\n",
      "✅ Model saved with Test Accuracy: 98.67% and bestepoch: 8\n",
      "Epoch [9/10], Test Accuracy: 98.57%, Classification Loss: 1.4614\n",
      "Epoch [10/10], Test Accuracy: 98.66%, Classification Loss: 1.4617\n"
     ]
    }
   ],
   "execution_count": 54
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 245
    },
    "collapsed": true,
    "id": "ZGgrhEnK937q",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "c6931eed-5980-49ac-ca9d-909a9e42bc04",
    "ExecuteTime": {
     "end_time": "2025-03-28T15:34:35.976942Z",
     "start_time": "2025-03-28T15:34:35.764002Z"
    }
   },
   "source": [
    "model_finetune2 = WatermarkedCNN().to(device)  # Assuming WatermarkedCNN is your model class\n",
    "\n",
    "model_finetune2.load_state_dict(torch.load('0best_finetuned_model_lR:0.0001_Acc:98.67_Epoch:8.pth'))\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "      conv2_mean = model_finetune2.conv2.weight.mean(dim=0)  # Should be (1,3,3)\n",
    "      conv2_mean_flat = conv2_mean.view(-1)\n",
    "      extracted_watermark = torch.sigmoid(torch.matmul( wm_regularizer.secret_key.cpu(), conv2_mean_flat.cpu()))  # Fix dimensions\n",
    "      extracted_watermark_binary = (extracted_watermark > 0.5).float()\n",
    "\n",
    "      # print(type(extracted_watermark_binary))\n",
    "      # print(type(watermark_vector))\n",
    "\n",
    "      print(\"\\nOriginal Watermark:\", watermark_vector.cpu().numpy())\n",
    "      print(\"\\nExtracted Watermark:\", extracted_watermark_binary.cpu().numpy())\n",
    "      print(\"\\nBER\", compute_ber(watermark_vector.cpu(), extracted_watermark_binary.cpu()))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Watermark: [0. 1. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Extracted Watermark: [0. 1. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "BER 0.0\n"
     ]
    }
   ],
   "execution_count": 59
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VVNnuZRHmeD"
   },
   "source": [
    "#**base:NOT embeded model fine tuning without embeding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XKLfYOZCHoKS"
   },
   "outputs": [],
   "source": [
    "LR=1e-3\n",
    "model_finetune = WatermarkedCNN(watermark_vector).to(device)  # Assuming WatermarkedCNN is your model class\n",
    "model_finetune.load_state_dict(torch.load('/content/best_model_WITHHOUT_watermark_lR:0.001_Acc:98.51_Epoch:8.pth'))\n",
    "model_finetune.fc2 = nn.Linear(512, 10).to(device)\n",
    "model_finetune.train()\n",
    "optimizer = optim.Adam(model_finetune.parameters(), lr=LR)  #fine tune all parameteres\n",
    "# optimizer = optim.Adam(model_finetune.fc2.parameters(), lr=LR) #finetune just last layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GVayqGy7H_Hl",
    "outputId": "6ccfe99f-d253-494b-e89f-e918e67549f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Test Accuracy: 98.00%, Classification Loss: 1.4699\n",
      "✅ Model saved with Test Accuracy: 98.00% and bestepoch: 1\n",
      "Epoch [2/10], Test Accuracy: 98.00%, Classification Loss: 1.4821\n",
      "Epoch [3/10], Test Accuracy: 98.00%, Classification Loss: 1.4812\n",
      "Epoch [4/10], Test Accuracy: 98.00%, Classification Loss: 1.4663\n",
      "Epoch [5/10], Test Accuracy: 98.00%, Classification Loss: 1.4617\n",
      "Epoch [6/10], Test Accuracy: 98.00%, Classification Loss: 1.4612\n",
      "Epoch [7/10], Test Accuracy: 98.00%, Classification Loss: 1.4619\n",
      "Epoch [8/10], Test Accuracy: 98.00%, Classification Loss: 1.4695\n",
      "Epoch [9/10], Test Accuracy: 98.00%, Classification Loss: 1.4612\n",
      "Epoch [10/10], Test Accuracy: 98.00%, Classification Loss: 1.4716\n"
     ]
    }
   ],
   "source": [
    "num_finetune_epochs = 10  # Adjust as needed\n",
    "best_finetune_acc = 0.0  # Track best test accuracy\n",
    "best_model_path = \"best_model_finetune.pth\"\n",
    "for epoch in range(num_finetune_epochs):\n",
    "    model_finetune.train()  # Ensure model is in training mode\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model_finetune(images)\n",
    "        loss = criterion(outputs, labels)  # Compute loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "    model_finetune.eval()  # Set model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():  # No gradients needed\n",
    "          for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model_finetune(images)\n",
    "            _, predicted = torch.max(outputs, 1)  # Get class index with highest probability\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()  # Count correct predictions\n",
    "\n",
    "    accuracy = 100 * correct / total  # Compute accuracy percentage\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_finetune_epochs}], Test Accuracy: {accuracy:.2f}%, Classification Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # Save the fine-tuned model\n",
    "    if accuracy>best_finetune_acc :\n",
    "        if os.path.exists(best_model_path):\n",
    "          os.remove(best_model_path)\n",
    "        best_finetune_acc =accuracy\n",
    "        best_epoch=epoch+1\n",
    "        best_model_path = f\"best_finetuned_model_lR:{LR}_Acc:{best_finetune_acc}_Epoch:{best_epoch}.pth\"\n",
    "        torch.save(model_finetune.state_dict(), best_model_path)\n",
    "        print(f\"✅ Model saved with Test Accuracy: {best_finetune_acc:.2f}% and bestepoch: {best_epoch}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PN8fNFCxIBdE",
    "outputId": "222a3898-7f88-4037-b981-3e2cb0fd3cb2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Watermark: [0. 1. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Extracted Watermark: [0. 1. 1. 0. 1. 0. 0. 0.]\n",
      "\n",
      "BER 0.375\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "      conv2_mean = model_finetune.conv2.weight.mean(dim=0)  # Should be (1,3,3)\n",
    "      conv2_mean_flat = conv2_mean.view(-1)\n",
    "      extracted_watermark = torch.sigmoid(torch.matmul( wm_regularizer.secret_key.cpu(), conv2_mean_flat.cpu()))  # Fix dimensions\n",
    "      extracted_watermark_binary = (extracted_watermark > 0.5).float()\n",
    "\n",
    "      # print(type(extracted_watermark_binary))\n",
    "      # print(type(watermark_vector))\n",
    "\n",
    "      print(\"\\nOriginal Watermark:\", watermark_vector.cpu().numpy())\n",
    "      print(\"\\nExtracted Watermark:\", extracted_watermark_binary.cpu().numpy())\n",
    "      print(\"\\nBER\", compute_ber(watermark_vector.cpu(), extracted_watermark_binary.cpu()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JyTNPP90MxsL"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AvjhGtvlMy-e"
   },
   "source": [
    "# **base embeded--- model fine tune with same embeding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I7KnNygbM1Ct"
   },
   "outputs": [],
   "source": [
    "LR=1e-3\n",
    "model_finetune = WatermarkedCNN().to(device)  # Assuming WatermarkedCNN is your model class\n",
    "model_finetune.load_state_dict(torch.load('/content/BASE_best_model_WITH_watermark_lR:0.001_lamda:0.01_Acc:98.46_Epoch:9_Ber:0.0.pth'))\n",
    "model_finetune.fc2 = nn.Linear(512, 10).to(device)\n",
    "model_finetune.train()\n",
    "optimizer = optim.Adam(model_finetune.parameters(), lr=LR)  #fine tune all parameteres\n",
    "# optimizer = optim.Adam(model_finetune.fc2.parameters(), lr=1e-3) #finetune just last layer\n",
    "\n",
    "num_finetune_epochs = 10  # Adjust as needed\n",
    "best_finetune_acc = 0.0  # Track best test accuracy\n",
    "best_model_path = \"best_model_finetune.pth\"\n",
    "batch_size = 256\n",
    "lambda_wm =1e-2 # 1e-2 Watermark regularizer 0.01\n",
    "\n",
    "best_ber = float(\"inf\")  # Track lowest BER\n",
    "\n",
    "\n",
    "\n",
    "watermark_vector=torch.tensor([0., 1., 0., 1., 1., 0., 0., 1.], dtype=torch.float32)\n",
    "\n",
    "# print(watermark_vector)\n",
    "# print(watermark_vector.size()) #8\n",
    "\n",
    "\n",
    "\n",
    "wm_regularizer = WatermarkRegularizer(lambda_wm, watermark_vector, C_in=1, K=3)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "# Training the model\n",
    "# import numpy as np\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model_finetune.train()\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model_finetune(images)\n",
    "        loss_class = criterion(outputs, labels)\n",
    "        wm_loss = wm_regularizer(model_finetune.conv2.weight)\n",
    "\n",
    "        loss = loss_class + wm_loss  # Total loss\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "    # current_lr = optimizer.param_groups[0]['lr']\n",
    "    # print(f\"Epoch {epoch+1}/{num_epochs}, Learning Rate: {current_lr:.6f}\")\n",
    "    # scheduler.step()\n",
    "\n",
    "\n",
    "    model_finetune.eval()  # Set model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():  # No gradients needed\n",
    "          for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model_finetune(images)\n",
    "            _, predicted = torch.max(outputs, 1)  # Get class index with highest probability\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()  # Count correct predictions\n",
    "\n",
    "    accuracy = 100 * correct / total  # Compute accuracy percentage\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Test Accuracy: {accuracy:.2f}%, Classification Loss: {loss_class.item():.4f}, \"\n",
    "          f\"Watermark Loss: {wm_loss.item():.4f}\")\n",
    "    # print(f\"Epoch [{epoch+1}/{num_epochs}],  Classification Loss: {loss_class.item():.4f}, \"\n",
    "    #       f\"Watermark Loss: {wm_loss.item():.4f}\")\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "      conv2_mean = model_finetune.conv2.weight.mean(dim=0)  # Should be (1,3,3)\n",
    "      conv2_mean_flat = conv2_mean.view(-1)\n",
    "      extracted_watermark = torch.sigmoid(torch.matmul( wm_regularizer.secret_key.cpu(), conv2_mean_flat.cpu()))  # Fix dimensions\n",
    "      extracted_watermark_binary = (extracted_watermark > 0.5).float()\n",
    "\n",
    "      # print(type(extracted_watermark_binary))\n",
    "      # print(type(watermark_vector))\n",
    "\n",
    "      print(\"\\nOriginal Watermark:\", watermark_vector.cpu().numpy())\n",
    "      print(\"\\nExtracted Watermark:\", extracted_watermark_binary.cpu().numpy())\n",
    "      print(\"\\nBER\", compute_ber(watermark_vector.cpu(), extracted_watermark_binary.cpu()))\n",
    "\n",
    "\n",
    "    ber=compute_ber(watermark_vector.cpu(), extracted_watermark_binary.cpu())\n",
    "    if accuracy>best_finetune_acc and ber<=best_ber:\n",
    "        if os.path.exists(best_model_path):\n",
    "          os.remove(best_model_path)\n",
    "        best_acc=accuracy\n",
    "        best_ber=ber\n",
    "        best_epoch=epoch+1\n",
    "        best_model_path = f\"best_model_lR:{learning_rate}_lamda:{lambda_wm}_Acc:{best_finetune_acc}_Epoch:{best_epoch}_Ber:{best_ber}.pth\"\n",
    "        torch.save(model_finetune.state_dict(), best_model_path)\n",
    "        print(f\"✅ Model saved with Test Accuracy: {best_acc:.2f}% and BER: {best_ber:.4f} and bestepoch: {best_epoch}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Extract the watermark after training\n",
    "# with torch.no_grad():\n",
    "#     conv2_mean = model.conv2.weight.mean(dim=0)  # Should be (1,3,3)\n",
    "#     conv2_mean_flat = conv2_mean.view(-1)\n",
    "#     extracted_watermark = torch.sigmoid(torch.matmul( wm_regularizer.secret_key, conv2_mean_flat))  # Fix dimensions\n",
    "#     extracted_watermark_binary = (extracted_watermark > 0.5).float()\n",
    "\n",
    "\n",
    "# Display results\n",
    "# print(\"\\nOriginal Watermark:\", watermark_vector.cpu().numpy())\n",
    "# print(\"\\nExtracted Watermark:\", extracted_watermark_binary.cpu().numpy())\n",
    "\n",
    "print(f\"✅Final  Model Test Accuracy: {best_finetune_acc:.2f}% and BER: {best_ber:.4f} and bestepoch: {best_epoch}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O47etvCsNEYt"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "874PmbC4CRbq"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
