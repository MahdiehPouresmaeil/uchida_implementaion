{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": ""
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ST98LX_z8uHh"
   },
   "source": [
    "# **Base model with embedding watermark**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zcbTIZqIF7zS"
   },
   "source": [
    "\n",
    "Epoch [1/10], Test Accuracy: 64.96%, Classification Loss: 1.7705, Watermark Loss: 0.0070\n",
    "\n",
    "Original Watermark: [0. 1. 0. 1. 1. 0. 0. 1.]\n",
    "\n",
    "Extracted Watermark: [1. 1. 0. 0. 0. 1. 1. 0.]\n",
    "\n",
    "BER 0.75\n",
    "Epoch [2/10], Test Accuracy: 97.11%, Classification Loss: 1.5053, Watermark Loss: 0.0069\n",
    "\n",
    "Original Watermark: [0. 1. 0. 1. 1. 0. 0. 1.]\n",
    "\n",
    "Extracted Watermark: [0. 1. 0. 0. 0. 0. 1. 0.]\n",
    "\n",
    "BER 0.5\n",
    "Epoch [3/10], Test Accuracy: 97.67%, Classification Loss: 1.4900, Watermark Loss: 0.0068\n",
    "\n",
    "Original Watermark: [0. 1. 0. 1. 1. 0. 0. 1.]\n",
    "\n",
    "Extracted Watermark: [0. 1. 0. 0. 1. 0. 1. 0.]\n",
    "\n",
    "BER 0.375\n",
    "Epoch [4/10], Test Accuracy: 98.05%, Classification Loss: 1.4667, Watermark Loss: 0.0068\n",
    "\n",
    "Original Watermark: [0. 1. 0. 1. 1. 0. 0. 1.]\n",
    "\n",
    "Extracted Watermark: [0. 1. 0. 0. 1. 0. 1. 0.]\n",
    "\n",
    "BER 0.375\n",
    "Epoch [5/10], Test Accuracy: 98.00%, Classification Loss: 1.4613, Watermark Loss: 0.0067\n",
    "\n",
    "Original Watermark: [0. 1. 0. 1. 1. 0. 0. 1.]\n",
    "\n",
    "Extracted Watermark: [0. 1. 0. 0. 1. 0. 1. 0.]\n",
    "\n",
    "BER 0.375\n",
    "Epoch [6/10], Test Accuracy: 98.17%, Classification Loss: 1.4720, Watermark Loss: 0.0066\n",
    "\n",
    "Original Watermark: [0. 1. 0. 1. 1. 0. 0. 1.]\n",
    "\n",
    "Extracted Watermark: [0. 1. 0. 1. 1. 0. 1. 0.]\n",
    "\n",
    "BER 0.25\n",
    "Epoch [7/10], Test Accuracy: 98.36%, Classification Loss: 1.4615, Watermark Loss: 0.0066\n",
    "\n",
    "Original Watermark: [0. 1. 0. 1. 1. 0. 0. 1.]\n",
    "\n",
    "Extracted Watermark: [0. 1. 0. 1. 1. 0. 0. 1.]\n",
    "\n",
    "BER 0.0\n",
    "✅ Model saved with Test Accuracy: 98.36% and BER: 0.0000 and bestepoch: 7\n",
    "Epoch [8/10], Test Accuracy: 98.38%, Classification Loss: 1.4703, Watermark Loss: 0.0065\n",
    "\n",
    "Original Watermark: [0. 1. 0. 1. 1. 0. 0. 1.]\n",
    "\n",
    "Extracted Watermark: [0. 1. 0. 1. 1. 0. 0. 1.]\n",
    "\n",
    "BER 0.0\n",
    "✅ Model saved with Test Accuracy: 98.38% and BER: 0.0000 and bestepoch: 8\n",
    "Epoch [9/10], Test Accuracy: 98.46%, Classification Loss: 1.4671, Watermark Loss: 0.0064\n",
    "\n",
    "Original Watermark: [0. 1. 0. 1. 1. 0. 0. 1.]\n",
    "\n",
    "Extracted Watermark: [0. 1. 0. 1. 1. 0. 0. 1.]\n",
    "\n",
    "BER 0.0\n",
    "✅ Model saved with Test Accuracy: 98.46% and BER: 0.0000 and bestepoch: 9\n",
    "Epoch [10/10], Test Accuracy: 98.42%, Classification Loss: 1.4612, Watermark Loss: 0.0064\n",
    "\n",
    "Original Watermark: [0. 1. 0. 1. 1. 0. 0. 1.]\n",
    "\n",
    "Extracted Watermark: [0. 1. 0. 1. 1. 0. 0. 1.]\n",
    "\n",
    "BER 0.0\n",
    "✅Final  Model Test Accuracy: 98.46% and BER: 0.0000 and bestepoch: 9\n",
    "\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-31T07:12:35.992219Z",
     "start_time": "2025-03-31T07:12:35.983450Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "\n",
    "import torch.optim.lr_scheduler as lr_scheduler"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-31T07:12:38.035982Z",
     "start_time": "2025-03-31T07:12:38.021394Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Set random seed for reproducibility\n",
    "seed = 58  # You can choose any number\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)  # If using multiple GPUs\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.backends.cudnn.deterministic = True  # Ensure deterministic behavior\n",
    "torch.backends.cudnn.benchmark = False  # Disable benchmark for reproducibility\n"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-31T07:12:39.580955Z",
     "start_time": "2025-03-31T07:12:39.573577Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-31T07:12:41.120598Z",
     "start_time": "2025-03-31T07:12:41.111921Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Hyperparameters\n",
    "num_epochs =10\n",
    "batch_size = 256\n",
    "learning_rate =0.001\n",
    "lambda_wm =1e-2 # 1e-2 Watermark regularizer 0.01\n",
    "best_acc = 0.0  # Track best test accuracy\n",
    "best_ber = float(\"inf\")  # Track lowest BER\n",
    "best_model_path = \"best_model.pth\"\n"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-31T07:12:42.715580Z",
     "start_time": "2025-03-31T07:12:42.584846Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load dataset\n",
    "\n",
    "\n",
    "# !rm -rf ./data/MNIST\n",
    "\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    # transforms.RandomRotation(10),  # Rotate images by ±10 degrees\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, transform=transform, download=True)  # Test set\n",
    "\n",
    "# Use the same seed for the DataLoader shuffle\n",
    "g = torch.Generator()\n",
    "g.manual_seed(seed)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, worker_init_fn=lambda _: np.random.seed(seed), generator=g)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-31T07:12:45.876855Z",
     "start_time": "2025-03-31T07:12:45.870891Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Computes the Bit Error Rate (BER)\n",
    "def compute_ber(original_watermark, extracted_watermark):\n",
    "  diff = original_watermark - extracted_watermark\n",
    "  num_errors = torch.sum(torch.abs(diff))\n",
    "  ber = num_errors.float() / original_watermark.numel()\n",
    "  return ber.item() #float\n"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-31T07:12:49.692250Z",
     "start_time": "2025-03-31T07:12:49.679574Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define Watermark Regularizer\n",
    "class WatermarkRegularizer(nn.Module):\n",
    "    def __init__(self, lambda_wm, watermark_vector, C_in, K):\n",
    "        super(WatermarkRegularizer, self).__init__()\n",
    "        self.lambda_wm = lambda_wm  # Watermark regularizer\n",
    "        self.watermark_vector = watermark_vector  #  watermark vector\n",
    "        T=watermark_vector.shape[0]\n",
    "        M = C_in*K*K  # Hidden dimension for projection\n",
    "        self.secret_key = torch.randn(T, M, device=device)\n",
    "        # self.secret_key = torch.nn.functional.normalize(self.secret_key, p=2, dim=1)\n",
    "        # print(self.secret_key.shape)   #8,9\n",
    "\n",
    "    def forward(self, weights):\n",
    "        # print(weights.size())   #32,1,3,3\n",
    "        w_mean = weights.mean(dim=(0))  # mean of filters\n",
    "        # print(w_mean.size())   #1*3*3\n",
    "        w_mean_flat = w_mean.view(-1)  # Flatten(C_in * K * K)\n",
    "        # print(w_mean_flat.size())   #9\n",
    "        # print(self.secret_key.T.size())  #9,8\n",
    "        projected_wm = torch.sigmoid(torch.matmul (self.secret_key,w_mean_flat))  # Compute WX\n",
    "        # wm_loss=self.lambda_wm * torch.norm(projected_wm - self.watermark_vector)  # Regularization loss\n",
    "        wm_loss = self.lambda_wm * nn.BCELoss(reduction='mean')(projected_wm.to(device), self.watermark_vector.to(device))\n",
    "        # print((projected_wm > 0.5).float())\n",
    "        # print(self.watermark_vector)\n",
    "        # print('***************************************')\n",
    "\n",
    "        return wm_loss"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-31T07:12:53.394402Z",
     "start_time": "2025-03-31T07:12:53.379120Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define a Simple CNN with Watermark Embedding\n",
    "class WatermarkedCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(WatermarkedCNN, self).__init__()\n",
    "        # self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        # self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(32 * 28 *28, 512)\n",
    "        self.fc2 = nn.Linear(512, 10)\n",
    "        # self.wm_regularizer = WatermarkRegularizer(lambda_wm, watermark_vector, C_in=1, K=3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv2(x)) # watermark is here\n",
    "        # print(x.shape)\n",
    "        # wm_loss = self.wm_regularizer(self.conv2.weight)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # print(x.shape)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        # x=torch.sigmoid(x)\n",
    "        x = torch.softmax(x, dim=1)\n",
    "        return x#, wm_loss\n"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# generate a random watermark, ignoring the seed\n",
    "# rand_gen = torch.Generator(device)  # Create a new generator\n",
    "# rand_gen.seed()  # Seed it randomly\n",
    "\n",
    "# # Generate a new random watermark vector using this independent generator\n",
    "# watermark_vector = torch.randint(0, 2, (256,), dtype=torch.float32, device=device, generator=rand_gen)\n",
    "\n",
    "\n",
    "# watermark_vector = torch.randint(0, 2, (256,), dtype=torch.float32, device=device)  # Binary watermark\n",
    "\n",
    "\n",
    "\n",
    "watermark_vector=torch.tensor([0., 1., 0., 1., 1., 0., 0., 1.], dtype=torch.float32)\n",
    "\n",
    "# print(watermark_vector)\n",
    "# print(watermark_vector.size()) #8"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "NocXpnQb4Ych",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "adf0de91-755b-43f0-f206-ea10d867569f",
    "ExecuteTime": {
     "end_time": "2025-03-28T13:58:55.101816Z",
     "start_time": "2025-03-28T13:56:25.746038Z"
    }
   },
   "source": [
    "\n",
    "#BASE_EMBEDDED_MODEL\n",
    "\n",
    "model = WatermarkedCNN().to(device)\n",
    "wm_regularizer = WatermarkRegularizer(lambda_wm, watermark_vector, C_in=1, K=3)\n",
    "torch.save(wm_regularizer.secret_key, \"secret_key.pth\")  # Save the secret key\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss_class = criterion(outputs, labels)\n",
    "        wm_loss = wm_regularizer(model.conv2.weight)\n",
    "        loss = loss_class + wm_loss  # Total loss\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # current_lr = optimizer.param_groups[0]['lr']\n",
    "    # print(f\"Epoch {epoch+1}/{num_epochs}, Learning Rate: {current_lr:.6f}\")\n",
    "    # scheduler.step()\n",
    "\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():  # No gradients needed\n",
    "          for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)  # Get class index with highest probability\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()  # Count correct predictions\n",
    "    accuracy = 100 * correct / total  # Compute accuracy percentage\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Test Accuracy: {accuracy:.2f}%, Classification Loss: {loss_class.item():.4f}, \"\n",
    "          f\"Watermark Loss: {wm_loss.item():.4f}\")\n",
    "    # print(f\"Epoch [{epoch+1}/{num_epochs}],  Classification Loss: {loss_class.item():.4f}, \"\n",
    "    #       f\"Watermark Loss: {wm_loss.item():.4f}\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "      conv2_mean = model.conv2.weight.mean(dim=0)  # Should be (1,3,3)\n",
    "      conv2_mean_flat = conv2_mean.view(-1)\n",
    "      extracted_watermark = torch.sigmoid(torch.matmul( wm_regularizer.secret_key.cpu(), conv2_mean_flat.cpu()))  # Fix dimensions\n",
    "      extracted_watermark_binary = (extracted_watermark > 0.5).float()\n",
    "      # print(type(extracted_watermark_binary))\n",
    "      # print(type(watermark_vector))\n",
    "      print(\"\\nOriginal Watermark:\", watermark_vector.cpu().numpy())\n",
    "      print(\"\\nExtracted Watermark:\", extracted_watermark_binary.cpu().numpy())\n",
    "      print(\"\\nBER\", compute_ber(watermark_vector.cpu(), extracted_watermark_binary.cpu()))\n",
    "\n",
    "    ber=compute_ber(watermark_vector.cpu(), extracted_watermark_binary.cpu())\n",
    "    if accuracy>best_acc and ber<=0.0:\n",
    "        if os.path.exists(best_model_path):\n",
    "          os.remove(best_model_path)\n",
    "        best_acc=accuracy\n",
    "        best_ber=ber\n",
    "        best_epoch=epoch+1\n",
    "        best_model_path = f\"best_model_lR:{learning_rate}_lamda:{lambda_wm}_Acc:{best_acc}_Epoch:{best_epoch}_Ber:{best_ber}.pth\"\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        print(f\"✅ Model saved with Test Accuracy: {best_acc:.2f}% and BER: {best_ber:.4f} and bestepoch: {best_epoch}\")\n",
    "\n",
    "print(f\"✅Final  Model Test Accuracy: {best_acc:.2f}% and BER: {best_ber:.4f} and bestepoch: {best_epoch}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Test Accuracy: 86.15%, Classification Loss: 1.5913, Watermark Loss: 0.0068\n",
      "\n",
      "Original Watermark: [0. 1. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Extracted Watermark: [0. 1. 1. 1. 1. 1. 1. 0.]\n",
      "\n",
      "BER 0.5\n",
      "Epoch [2/10], Test Accuracy: 87.35%, Classification Loss: 1.6720, Watermark Loss: 0.0066\n",
      "\n",
      "Original Watermark: [0. 1. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Extracted Watermark: [0. 1. 1. 1. 1. 1. 0. 1.]\n",
      "\n",
      "BER 0.25\n",
      "Epoch [3/10], Test Accuracy: 88.01%, Classification Loss: 1.5680, Watermark Loss: 0.0065\n",
      "\n",
      "Original Watermark: [0. 1. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Extracted Watermark: [0. 1. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "BER 0.0\n",
      "✅ Model saved with Test Accuracy: 88.01% and BER: 0.0000 and bestepoch: 3\n",
      "Epoch [4/10], Test Accuracy: 97.98%, Classification Loss: 1.4762, Watermark Loss: 0.0064\n",
      "\n",
      "Original Watermark: [0. 1. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Extracted Watermark: [0. 1. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "BER 0.0\n",
      "✅ Model saved with Test Accuracy: 97.98% and BER: 0.0000 and bestepoch: 4\n",
      "Epoch [5/10], Test Accuracy: 98.09%, Classification Loss: 1.4861, Watermark Loss: 0.0063\n",
      "\n",
      "Original Watermark: [0. 1. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Extracted Watermark: [0. 1. 0. 0. 1. 0. 0. 1.]\n",
      "\n",
      "BER 0.125\n",
      "Epoch [6/10], Test Accuracy: 98.48%, Classification Loss: 1.4753, Watermark Loss: 0.0062\n",
      "\n",
      "Original Watermark: [0. 1. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Extracted Watermark: [0. 1. 0. 0. 1. 0. 0. 1.]\n",
      "\n",
      "BER 0.125\n",
      "Epoch [7/10], Test Accuracy: 98.31%, Classification Loss: 1.4621, Watermark Loss: 0.0061\n",
      "\n",
      "Original Watermark: [0. 1. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Extracted Watermark: [0. 1. 0. 0. 1. 0. 0. 1.]\n",
      "\n",
      "BER 0.125\n",
      "Epoch [8/10], Test Accuracy: 98.37%, Classification Loss: 1.4725, Watermark Loss: 0.0060\n",
      "\n",
      "Original Watermark: [0. 1. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Extracted Watermark: [0. 1. 0. 0. 1. 0. 0. 1.]\n",
      "\n",
      "BER 0.125\n",
      "Epoch [9/10], Test Accuracy: 98.30%, Classification Loss: 1.4715, Watermark Loss: 0.0059\n",
      "\n",
      "Original Watermark: [0. 1. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Extracted Watermark: [0. 1. 0. 0. 1. 0. 0. 1.]\n",
      "\n",
      "BER 0.125\n",
      "Epoch [10/10], Test Accuracy: 98.52%, Classification Loss: 1.4613, Watermark Loss: 0.0058\n",
      "\n",
      "Original Watermark: [0. 1. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Extracted Watermark: [0. 1. 0. 0. 1. 0. 0. 1.]\n",
      "\n",
      "BER 0.125\n",
      "✅Final  Model Test Accuracy: 97.98% and BER: 0.0000 and bestepoch: 4\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "874PmbC4CRbq"
   },
   "source": [
    "# **Base model without watermark**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "EbKW3MdVCRLG",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "67e12ebe-2b4d-4676-c8b0-051b7b158c67",
    "ExecuteTime": {
     "end_time": "2025-03-28T14:25:45.813804Z",
     "start_time": "2025-03-28T14:23:20.779860Z"
    }
   },
   "source": [
    "#BASE_MODEL_NOT_EMBEDED\n",
    "# # generate a random watermark, ignoring the seed\n",
    "# rand_gen = torch.Generator(device)  # Create a new generator\n",
    "# rand_gen.seed()  # Seed it randomly\n",
    "best_model_path = \"best_model2.pth\"\n",
    "\n",
    "\n",
    "model = WatermarkedCNN().to(device)\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss_class = criterion(outputs, labels)\n",
    "        loss = loss_class  # Total loss\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():  # No gradients needed\n",
    "          for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)  # Get class index with highest probability\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()  # Count correct predictions\n",
    "\n",
    "    accuracy = 100 * correct / total  # Compute accuracy percentage\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Test Accuracy: {accuracy:.2f}%, Classification Loss: {loss_class.item():.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "    if accuracy>best_acc :\n",
    "        if os.path.exists(best_model_path):\n",
    "          os.remove(best_model_path)\n",
    "        best_acc=accuracy\n",
    "        best_epoch=epoch+1\n",
    "        best_model_path = f\"best_model_lR:{learning_rate}_Acc:{best_acc}_Epoch:{best_epoch}.pth\"\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        print(f\"✅ Model saved with Test Accuracy: {best_acc:.2f}%  and bestepoch: {best_epoch}\")\n",
    "\n",
    "\n",
    "print(f\"✅Final  Model Test Accuracy: {best_acc:.2f}% and bestepoch: {best_epoch}\")\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Test Accuracy: 86.19%, Classification Loss: 1.5423\n",
      "Epoch [2/10], Test Accuracy: 87.02%, Classification Loss: 1.5721\n",
      "Epoch [3/10], Test Accuracy: 88.05%, Classification Loss: 1.5643\n",
      "Epoch [4/10], Test Accuracy: 97.81%, Classification Loss: 1.4625\n",
      "Epoch [5/10], Test Accuracy: 97.85%, Classification Loss: 1.4852\n",
      "Epoch [6/10], Test Accuracy: 98.49%, Classification Loss: 1.4727\n",
      "✅ Model saved with Test Accuracy: 98.49%  and bestepoch: 6\n",
      "Epoch [7/10], Test Accuracy: 98.46%, Classification Loss: 1.4615\n",
      "Epoch [8/10], Test Accuracy: 98.21%, Classification Loss: 1.4623\n",
      "Epoch [9/10], Test Accuracy: 98.52%, Classification Loss: 1.4716\n",
      "✅ Model saved with Test Accuracy: 98.52%  and bestepoch: 9\n",
      "Epoch [10/10], Test Accuracy: 98.19%, Classification Loss: 1.4877\n",
      "✅Final  Model Test Accuracy: 98.52% and bestepoch: 9\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TygIUrqq8j64"
   },
   "source": [
    "# **base:embeded model fine tuning without embeding- fine tune all parametres**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CQcmQHwyGgMy"
   },
   "source": [
    "Original Watermark: [0. 1. 0. 1. 1. 0. 0. 1.]\n",
    "\n",
    "Extracted Watermark: [0. 1. 0. 1. 1. 0. 0. 1.]\n",
    "\n",
    "BER 0.0"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "VTrqJljt5LXn",
    "ExecuteTime": {
     "end_time": "2025-03-31T08:44:19.143307Z",
     "start_time": "2025-03-31T08:44:18.915987Z"
    }
   },
   "source": [
    "#fine tunning with the dataSET to see watermark still exist or not\n",
    "rand_gen = torch.Generator(device)  # Create a new generator\n",
    "rand_gen.seed()  # Seed it randomly\n",
    "# Assuming you have saved your model as 'model.pth'\n",
    "LR=0.0001\n",
    "model_finetune = WatermarkedCNN().to(device)  # Assuming WatermarkedCNN is your model class\n",
    "model_finetune.load_state_dict(torch.load('BASE-EMBEDDED_MODEL_lR:0.001_lamda:0.01_Acc:97.98_Epoch:4_Ber:0.0.pth'))\n",
    "# model_finetune.fc2 = nn.Linear(512, 10).to(device)\n",
    "model_finetune.train()\n",
    "\n",
    "\n",
    "# for name, param in model_finetune.named_parameters():\n",
    "#     print(f\"{name}: requires_grad={param.requires_grad}\")\n",
    "\n",
    "optimizer = optim.Adam(model_finetune.parameters(), lr=LR)  #fine tune all parameteres\n",
    "# optimizer = optim.Adam(model_finetune.fc2.parameters(), lr=1e-3) #finetune just last layer\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 76
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jMq-ny7b72aZ",
    "outputId": "3fd532ee-d54b-4f33-b5fc-673761978f6a",
    "ExecuteTime": {
     "end_time": "2025-03-31T08:44:40.302491Z",
     "start_time": "2025-03-31T08:44:20.729628Z"
    }
   },
   "source": [
    "\n",
    "num_finetune_epochs = 10  # Adjust as needed\n",
    "best_finetune_acc = 0.0  # Track best test accuracy\n",
    "best_model_path = \"best_model_finetune.pth\"\n",
    "for epoch in range(num_finetune_epochs):\n",
    "    model_finetune.train()  # Ensure model is in training mode\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model_finetune(images)\n",
    "\n",
    "        loss = criterion(outputs, labels)  # Compute loss\n",
    "        loss.backward()\n",
    "\n",
    "        # for name, param in model_finetune.named_parameters():\n",
    "        #   if param.requires_grad and param.grad is not None:\n",
    "        #       print(f\"{name} - Grad Norm: {param.grad.norm().item()}\")\n",
    "        #\n",
    "        #\n",
    "        # for name, param in model_finetune.named_parameters():\n",
    "        #    if param.requires_grad:\n",
    "        #       print(f\"{name} - Grad After Backward: {param.grad}\")\n",
    "\n",
    "        # for param_group in optimizer.param_groups:\n",
    "        #     print(param_group['lr'])\n",
    "\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        # print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
    "\n",
    "\n",
    "\n",
    "    model_finetune.eval()  # Set model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():  # No gradients needed\n",
    "          for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs =model_finetune(images)\n",
    "            _, predicted = torch.max(outputs, 1)  # Get class index with highest probability\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()  # Count correct predictions\n",
    "\n",
    "    accuracy = 100 * correct / total  # Compute accuracy percentage\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_finetune_epochs}], Test Accuracy: {accuracy:.2f}%, Classification Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # Save the fine-tuned model\n",
    "    if accuracy>best_finetune_acc :\n",
    "        if os.path.exists(best_model_path):\n",
    "          os.remove(best_model_path)\n",
    "        best_finetune_acc =accuracy\n",
    "        best_epoch=epoch+1\n",
    "        best_model_path = f\"best_finetuned_model_lR:{LR}_Acc:{best_finetune_acc}_Epoch:{best_epoch}.pth\"\n",
    "        torch.save(model_finetune.state_dict(), best_model_path)\n",
    "        print(f\"✅ Model saved with Test Accuracy: {best_finetune_acc:.2f}% and bestepoch: {best_epoch}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Test Accuracy: 98.43%, Classification Loss: 1.4705\n",
      "✅ Model saved with Test Accuracy: 98.43% and bestepoch: 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[77]\u001B[39m\u001B[32m, line 7\u001B[39m\n\u001B[32m      4\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(num_finetune_epochs):\n\u001B[32m      5\u001B[39m     model_finetune.train()  \u001B[38;5;66;03m# Ensure model is in training mode\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m7\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mimages\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m      8\u001B[39m \u001B[43m        \u001B[49m\u001B[43mimages\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43mimages\u001B[49m\u001B[43m.\u001B[49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m.\u001B[49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     10\u001B[39m \u001B[43m        \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m.\u001B[49m\u001B[43mzero_grad\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/uchida_implementaion/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:708\u001B[39m, in \u001B[36m_BaseDataLoaderIter.__next__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    705\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    706\u001B[39m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[32m    707\u001B[39m     \u001B[38;5;28mself\u001B[39m._reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m708\u001B[39m data = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    709\u001B[39m \u001B[38;5;28mself\u001B[39m._num_yielded += \u001B[32m1\u001B[39m\n\u001B[32m    710\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[32m    711\u001B[39m     \u001B[38;5;28mself\u001B[39m._dataset_kind == _DatasetKind.Iterable\n\u001B[32m    712\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m._IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    713\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m._num_yielded > \u001B[38;5;28mself\u001B[39m._IterableDataset_len_called\n\u001B[32m    714\u001B[39m ):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/uchida_implementaion/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:764\u001B[39m, in \u001B[36m_SingleProcessDataLoaderIter._next_data\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    762\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[32m    763\u001B[39m     index = \u001B[38;5;28mself\u001B[39m._next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m764\u001B[39m     data = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[32m    765\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._pin_memory:\n\u001B[32m    766\u001B[39m         data = _utils.pin_memory.pin_memory(data, \u001B[38;5;28mself\u001B[39m._pin_memory_device)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/uchida_implementaion/.venv/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:52\u001B[39m, in \u001B[36m_MapDatasetFetcher.fetch\u001B[39m\u001B[34m(self, possibly_batched_index)\u001B[39m\n\u001B[32m     50\u001B[39m         data = \u001B[38;5;28mself\u001B[39m.dataset.__getitems__(possibly_batched_index)\n\u001B[32m     51\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m---> \u001B[39m\u001B[32m52\u001B[39m         data = [\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[32m     53\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m     54\u001B[39m     data = \u001B[38;5;28mself\u001B[39m.dataset[possibly_batched_index]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/uchida_implementaion/.venv/lib/python3.12/site-packages/torchvision/datasets/mnist.py:146\u001B[39m, in \u001B[36mMNIST.__getitem__\u001B[39m\u001B[34m(self, index)\u001B[39m\n\u001B[32m    143\u001B[39m img = Image.fromarray(img.numpy(), mode=\u001B[33m\"\u001B[39m\u001B[33mL\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m    145\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.transform \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m146\u001B[39m     img = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mtransform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    148\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.target_transform \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    149\u001B[39m     target = \u001B[38;5;28mself\u001B[39m.target_transform(target)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/uchida_implementaion/.venv/lib/python3.12/site-packages/torchvision/transforms/transforms.py:95\u001B[39m, in \u001B[36mCompose.__call__\u001B[39m\u001B[34m(self, img)\u001B[39m\n\u001B[32m     93\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, img):\n\u001B[32m     94\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m.transforms:\n\u001B[32m---> \u001B[39m\u001B[32m95\u001B[39m         img = \u001B[43mt\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     96\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m img\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/uchida_implementaion/.venv/lib/python3.12/site-packages/torchvision/transforms/transforms.py:137\u001B[39m, in \u001B[36mToTensor.__call__\u001B[39m\u001B[34m(self, pic)\u001B[39m\n\u001B[32m    129\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, pic):\n\u001B[32m    130\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    131\u001B[39m \u001B[33;03m    Args:\u001B[39;00m\n\u001B[32m    132\u001B[39m \u001B[33;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    135\u001B[39m \u001B[33;03m        Tensor: Converted image.\u001B[39;00m\n\u001B[32m    136\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m137\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[43m.\u001B[49m\u001B[43mto_tensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpic\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/uchida_implementaion/.venv/lib/python3.12/site-packages/torchvision/transforms/functional.py:168\u001B[39m, in \u001B[36mto_tensor\u001B[39m\u001B[34m(pic)\u001B[39m\n\u001B[32m    166\u001B[39m \u001B[38;5;66;03m# handle PIL Image\u001B[39;00m\n\u001B[32m    167\u001B[39m mode_to_nptype = {\u001B[33m\"\u001B[39m\u001B[33mI\u001B[39m\u001B[33m\"\u001B[39m: np.int32, \u001B[33m\"\u001B[39m\u001B[33mI;16\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m sys.byteorder == \u001B[33m\"\u001B[39m\u001B[33mlittle\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33mI;16B\u001B[39m\u001B[33m\"\u001B[39m: np.int16, \u001B[33m\"\u001B[39m\u001B[33mF\u001B[39m\u001B[33m\"\u001B[39m: np.float32}\n\u001B[32m--> \u001B[39m\u001B[32m168\u001B[39m img = torch.from_numpy(\u001B[43mnp\u001B[49m\u001B[43m.\u001B[49m\u001B[43marray\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpic\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode_to_nptype\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpic\u001B[49m\u001B[43m.\u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnp\u001B[49m\u001B[43m.\u001B[49m\u001B[43muint8\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcopy\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m)\n\u001B[32m    170\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m pic.mode == \u001B[33m\"\u001B[39m\u001B[33m1\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m    171\u001B[39m     img = \u001B[32m255\u001B[39m * img\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 77
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 245
    },
    "collapsed": true,
    "id": "ZGgrhEnK937q",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "c6931eed-5980-49ac-ca9d-909a9e42bc04",
    "ExecuteTime": {
     "end_time": "2025-03-31T08:37:12.138924Z",
     "start_time": "2025-03-31T08:37:11.766658Z"
    }
   },
   "source": [
    "model_finetune2 = WatermarkedCNN().to(device)  # Assuming WatermarkedCNN is your model class\n",
    "\n",
    "model_finetune2.load_state_dict(torch.load('0best_finetuned_model_lR:0.0001_Acc:98.67_Epoch:8.pth'))\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "      conv2_mean = model_finetune2.conv2.weight.mean(dim=0)  # Should be (1,3,3)\n",
    "      conv2_mean_flat = conv2_mean.view(-1)\n",
    "      extracted_watermark = torch.sigmoid(torch.matmul( wm_regularizer.secret_key.cpu(), conv2_mean_flat.cpu()))  # Fix dimensions\n",
    "      extracted_watermark_binary = (extracted_watermark > 0.5).float()\n",
    "\n",
    "      # print(type(extracted_watermark_binary))\n",
    "      # print(type(watermark_vector))\n",
    "\n",
    "      print(\"\\nOriginal Watermark:\", watermark_vector.cpu().numpy())\n",
    "      print(\"\\nExtracted Watermark:\", extracted_watermark_binary.cpu().numpy())\n",
    "      print(\"\\nBER\", compute_ber(watermark_vector.cpu(), extracted_watermark_binary.cpu()))\n",
    "\n",
    "#finish FINE_TUNING"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wm_regularizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[62]\u001B[39m\u001B[32m, line 9\u001B[39m\n\u001B[32m      7\u001B[39m conv2_mean = model_finetune2.conv2.weight.mean(dim=\u001B[32m0\u001B[39m)  \u001B[38;5;66;03m# Should be (1,3,3)\u001B[39;00m\n\u001B[32m      8\u001B[39m conv2_mean_flat = conv2_mean.view(-\u001B[32m1\u001B[39m)\n\u001B[32m----> \u001B[39m\u001B[32m9\u001B[39m extracted_watermark = torch.sigmoid(torch.matmul( \u001B[43mwm_regularizer\u001B[49m.secret_key.cpu(), conv2_mean_flat.cpu()))  \u001B[38;5;66;03m# Fix dimensions\u001B[39;00m\n\u001B[32m     10\u001B[39m extracted_watermark_binary = (extracted_watermark > \u001B[32m0.5\u001B[39m).float()\n\u001B[32m     12\u001B[39m \u001B[38;5;66;03m# print(type(extracted_watermark_binary))\u001B[39;00m\n\u001B[32m     13\u001B[39m \u001B[38;5;66;03m# print(type(watermark_vector))\u001B[39;00m\n",
      "\u001B[31mNameError\u001B[39m: name 'wm_regularizer' is not defined"
     ]
    }
   ],
   "execution_count": 62
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VVNnuZRHmeD"
   },
   "source": [
    "#**base:NOT embeded model fine tuning without embeding**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "XKLfYOZCHoKS",
    "ExecuteTime": {
     "end_time": "2025-03-31T10:01:16.557970Z",
     "start_time": "2025-03-31T10:01:16.342893Z"
    }
   },
   "source": [
    "#pruning the weights of the model\n",
    "\n",
    "\n",
    "rand_gen = torch.Generator(device)  # Create a new generator\n",
    "rand_gen.seed()  # Seed it randomly\n",
    "\n",
    "LR=0.0001\n",
    "model_prun= WatermarkedCNN().to(device)  # Assuming WatermarkedCNN is your model class\n",
    "model_prun.load_state_dict(torch.load('BASE-EMBEDDED_MODEL_lR:0.001_lamda:0.01_Acc:97.98_Epoch:4_Ber:0.0.pth'))\n",
    "# model_prun.eval()\n",
    "\n",
    "optimizer = optim.Adam(model_prun.parameters(), lr=LR)  #fine tune all parameteres\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Adam(model_finetune.fc2.parameters(), lr=1e-3) #finetune just last layer\n"
   ],
   "outputs": [],
   "execution_count": 239
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-31T10:01:18.272604Z",
     "start_time": "2025-03-31T10:01:18.266528Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import copy\n",
    "model_copy=copy.deepcopy(model_prun)"
   ],
   "outputs": [],
   "execution_count": 240
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-31T09:57:03.918687Z",
     "start_time": "2025-03-31T09:57:03.858897Z"
    }
   },
   "cell_type": "code",
   "source": [
    "state_dict1 = model_prun.state_dict()\n",
    "print(state_dict1)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict({'conv2.weight': tensor([[[[ 0.0364, -0.0118,  0.0811],\n",
      "          [-0.1568,  0.3304, -0.0164],\n",
      "          [-0.1316, -0.2825, -0.0853]]],\n",
      "\n",
      "\n",
      "        [[[-0.1040, -0.2836,  0.2635],\n",
      "          [-0.3641,  0.2070,  0.3110],\n",
      "          [-0.0376,  0.2994, -0.1587]]],\n",
      "\n",
      "\n",
      "        [[[-0.3878,  0.0688,  0.2889],\n",
      "          [ 0.3228, -0.0161, -0.0536],\n",
      "          [ 0.0370,  0.0169, -0.2963]]],\n",
      "\n",
      "\n",
      "        [[[-0.1488, -0.0051, -0.2002],\n",
      "          [ 0.3119, -0.1132, -0.3218],\n",
      "          [-0.1379,  0.3127,  0.3454]]],\n",
      "\n",
      "\n",
      "        [[[-0.0908, -0.1422,  0.3827],\n",
      "          [-0.0132, -0.2228, -0.0827],\n",
      "          [-0.0620, -0.2853, -0.2536]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1208,  0.2487, -0.1315],\n",
      "          [-0.0936,  0.3515, -0.1770],\n",
      "          [-0.3719, -0.2072,  0.2853]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0119,  0.2366, -0.4234],\n",
      "          [ 0.3459,  0.1551, -0.3175],\n",
      "          [-0.2677,  0.3744,  0.0044]]],\n",
      "\n",
      "\n",
      "        [[[-0.3824, -0.0374,  0.3677],\n",
      "          [-0.4045, -0.0400,  0.3591],\n",
      "          [-0.3629,  0.1721,  0.3986]]],\n",
      "\n",
      "\n",
      "        [[[-0.2342,  0.2580,  0.0681],\n",
      "          [-0.1964,  0.2692, -0.2517],\n",
      "          [-0.2692, -0.2057,  0.2146]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1858, -0.1921,  0.0092],\n",
      "          [ 0.0346,  0.3168,  0.0484],\n",
      "          [ 0.0707, -0.3152,  0.0260]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0903, -0.3383,  0.2329],\n",
      "          [-0.1666,  0.0232,  0.1146],\n",
      "          [ 0.2196, -0.1556,  0.2386]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0993,  0.1464, -0.1041],\n",
      "          [ 0.3322,  0.2558, -0.2181],\n",
      "          [-0.1354, -0.0900, -0.3926]]],\n",
      "\n",
      "\n",
      "        [[[-0.4456, -0.1936, -0.2947],\n",
      "          [ 0.3234,  0.0879,  0.1764],\n",
      "          [ 0.1751,  0.1549,  0.2845]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0905, -0.1282,  0.1093],\n",
      "          [ 0.2699, -0.2968,  0.0783],\n",
      "          [-0.2630,  0.1383,  0.2068]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2600, -0.0348,  0.1563],\n",
      "          [-0.2581,  0.2297,  0.0304],\n",
      "          [-0.0744, -0.2046,  0.1989]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2223, -0.3577, -0.0188],\n",
      "          [ 0.3612,  0.0123, -0.2740],\n",
      "          [-0.2179,  0.3154,  0.0466]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2958, -0.2353, -0.2281],\n",
      "          [-0.0042,  0.0231,  0.1964],\n",
      "          [-0.1537,  0.0730,  0.2331]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1443,  0.0011,  0.0582],\n",
      "          [ 0.2146, -0.1123, -0.3603],\n",
      "          [ 0.0316,  0.2913,  0.0062]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1797, -0.2224, -0.2168],\n",
      "          [-0.1130,  0.0820,  0.1508],\n",
      "          [-0.2644,  0.0636, -0.0416]]],\n",
      "\n",
      "\n",
      "        [[[-0.3396, -0.0935,  0.2001],\n",
      "          [-0.0889, -0.4084,  0.3871],\n",
      "          [ 0.3632, -0.4621, -0.0035]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1318, -0.2924, -0.1143],\n",
      "          [ 0.1350, -0.2438,  0.2317],\n",
      "          [-0.0180,  0.1759, -0.0139]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3488,  0.2954,  0.0038],\n",
      "          [-0.1510, -0.0632, -0.2597],\n",
      "          [-0.3132, -0.1682,  0.2509]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2536, -0.0698,  0.0310],\n",
      "          [ 0.0565,  0.2781, -0.2461],\n",
      "          [-0.2440,  0.1708,  0.3071]]],\n",
      "\n",
      "\n",
      "        [[[-0.0076,  0.2398, -0.0892],\n",
      "          [-0.0522,  0.0247,  0.1362],\n",
      "          [ 0.0949, -0.2257,  0.2979]]],\n",
      "\n",
      "\n",
      "        [[[-0.4562, -0.3624, -0.2584],\n",
      "          [ 0.1270,  0.1438, -0.1940],\n",
      "          [ 0.0290,  0.2018,  0.3358]]],\n",
      "\n",
      "\n",
      "        [[[-0.1258,  0.2391,  0.1505],\n",
      "          [-0.0680, -0.1241,  0.2273],\n",
      "          [-0.0722,  0.0356,  0.1606]]],\n",
      "\n",
      "\n",
      "        [[[-0.0844, -0.0931,  0.2466],\n",
      "          [ 0.0679,  0.1015, -0.0586],\n",
      "          [ 0.1014, -0.3171,  0.0123]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2912,  0.0146,  0.3701],\n",
      "          [ 0.2037, -0.1362,  0.0776],\n",
      "          [-0.4350, -0.0793, -0.3139]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0189,  0.1659, -0.2673],\n",
      "          [ 0.2411,  0.0399, -0.4245],\n",
      "          [ 0.2603,  0.1323, -0.1299]]],\n",
      "\n",
      "\n",
      "        [[[-0.1508, -0.4382, -0.4919],\n",
      "          [ 0.2844,  0.2056, -0.2191],\n",
      "          [ 0.1804,  0.4450, -0.0271]]],\n",
      "\n",
      "\n",
      "        [[[-0.3225,  0.0772,  0.2205],\n",
      "          [ 0.0214,  0.2696, -0.2199],\n",
      "          [ 0.1351, -0.0532,  0.1813]]],\n",
      "\n",
      "\n",
      "        [[[-0.3236, -0.2626,  0.2517],\n",
      "          [ 0.3895,  0.2517,  0.0322],\n",
      "          [-0.3845, -0.0904,  0.0857]]]], device='cuda:0'), 'conv2.bias': tensor([-0.0668, -0.2737, -0.0121, -0.1912, -0.2899, -0.1129, -0.2749, -0.3839,\n",
      "        -0.0775,  0.0780, -0.1741, -0.2623, -0.2566,  0.0672, -0.0241, -0.2911,\n",
      "        -0.2343, -0.1319,  0.1174, -0.2000, -0.0536, -0.0332, -0.5235, -0.3178,\n",
      "        -0.1966, -0.1683, -0.1102,  0.0084, -0.2077, -0.1248, -0.2715, -0.0953],\n",
      "       device='cuda:0'), 'fc1.weight': tensor([[ 6.2921e-03,  6.5319e-04,  8.3345e-03,  ...,  1.6364e-03,\n",
      "          5.7225e-03,  6.4337e-03],\n",
      "        [-5.4560e-03, -1.1619e-02, -1.2772e-02,  ..., -8.5252e-04,\n",
      "          1.7323e-04, -2.5367e-03],\n",
      "        [ 6.4434e-04,  2.3930e-03, -2.9082e-04,  ...,  4.0597e-03,\n",
      "          2.9580e-03,  9.1356e-03],\n",
      "        ...,\n",
      "        [-3.5319e-03, -5.4476e-03, -6.2750e-03,  ...,  1.0250e-03,\n",
      "          1.4246e-03, -1.3331e-02],\n",
      "        [-1.0681e-03, -3.3718e-03, -1.2866e-02,  ...,  4.3470e-03,\n",
      "         -5.6703e-05, -9.8906e-03],\n",
      "        [ 4.7858e-03, -7.3827e-04,  3.9747e-03,  ..., -3.3904e-03,\n",
      "          4.0162e-03, -5.0322e-03]], device='cuda:0'), 'fc1.bias': tensor([ 9.1664e-04, -7.3883e-03,  4.7045e-03, -3.4580e-03, -7.0109e-04,\n",
      "        -2.6502e-03, -9.5049e-03, -3.6318e-03, -3.3500e-03, -7.3026e-03,\n",
      "        -2.7604e-03, -3.0697e-03, -2.4907e-03, -8.3061e-05, -6.6043e-03,\n",
      "        -6.5028e-03, -6.6293e-03, -1.1812e-02, -3.8726e-03, -1.1445e-02,\n",
      "        -6.6641e-03, -7.6979e-04,  3.4689e-03,  6.0344e-03, -2.4584e-03,\n",
      "         6.6019e-04, -1.1930e-02,  5.0397e-03, -3.6771e-03,  5.6718e-03,\n",
      "        -9.7983e-03,  1.8007e-03,  3.4510e-03,  5.5427e-03, -5.0221e-03,\n",
      "        -1.0203e-02,  7.8270e-03, -9.1873e-03,  5.1802e-03, -1.1781e-02,\n",
      "        -1.2551e-03, -1.0751e-02, -8.4813e-03, -2.5257e-03, -3.9531e-03,\n",
      "         9.9554e-03, -2.9607e-03, -4.4712e-03, -1.9743e-03, -5.5505e-03,\n",
      "        -4.4989e-03,  1.3589e-03, -1.1464e-03, -1.7833e-02, -6.0800e-03,\n",
      "        -5.9040e-03, -1.0536e-02,  4.0676e-04, -3.9190e-03, -7.4208e-03,\n",
      "        -7.9080e-03,  3.7494e-03, -1.2207e-02, -5.8847e-03, -1.3969e-02,\n",
      "         1.0087e-02, -5.8543e-03, -1.5726e-03, -2.9552e-04, -4.2082e-03,\n",
      "        -1.3768e-02,  4.6927e-03, -1.3056e-02,  6.6451e-06, -1.0056e-02,\n",
      "         1.9423e-03, -5.0019e-03,  6.5177e-03, -1.9900e-02, -1.4369e-02,\n",
      "        -6.4286e-03, -3.0021e-03, -1.6095e-02, -8.9365e-03, -7.8872e-03,\n",
      "        -1.0617e-02,  5.9966e-03, -9.7813e-03,  3.0280e-03, -1.4019e-03,\n",
      "         9.3839e-03, -2.3876e-03,  1.7690e-02,  7.2536e-03, -4.4112e-03,\n",
      "         3.7963e-04, -1.0777e-02,  7.1859e-04, -5.9631e-04, -5.1836e-03,\n",
      "         3.5783e-03,  4.9494e-03, -2.7568e-03,  4.2356e-04,  1.7375e-02,\n",
      "         1.6300e-03, -5.5372e-03,  7.1471e-03, -1.0800e-02, -1.3204e-03,\n",
      "        -1.5432e-03,  1.7985e-03, -1.0802e-02, -5.2878e-03, -8.1139e-03,\n",
      "         1.6387e-03, -2.1402e-03, -1.6044e-02, -4.6161e-04, -6.2532e-03,\n",
      "        -7.3838e-03,  3.0542e-03,  3.8720e-03, -4.8145e-03, -8.7307e-03,\n",
      "        -6.4070e-04, -6.0897e-03,  7.0514e-04, -1.5766e-02, -9.1907e-03,\n",
      "        -6.5835e-03,  8.2501e-03, -3.7920e-03, -3.3250e-03,  6.9191e-03,\n",
      "        -1.1786e-02, -9.7649e-03,  1.7474e-04, -1.0657e-02, -2.3422e-03,\n",
      "         6.8983e-04,  5.4259e-03, -2.9513e-03, -2.7110e-03,  1.2518e-04,\n",
      "         1.8726e-03, -4.5285e-03, -7.2419e-03,  2.5138e-03,  2.7246e-03,\n",
      "         1.5209e-03,  6.7519e-04,  4.8255e-03, -3.4885e-03,  1.9061e-02,\n",
      "        -1.1192e-02, -2.1317e-03, -5.0357e-03, -4.7101e-03, -1.7442e-02,\n",
      "         5.9108e-03, -2.9846e-04, -3.7645e-03, -4.3752e-03, -1.1822e-02,\n",
      "        -5.5566e-04, -4.2594e-03, -2.9926e-03, -3.8177e-03, -3.2024e-03,\n",
      "         2.3376e-03, -2.0202e-03, -2.0474e-03,  3.3153e-03, -9.8287e-03,\n",
      "        -7.4668e-03, -5.6280e-03,  8.8980e-03,  5.6460e-03,  7.2448e-03,\n",
      "         6.0470e-03, -1.0323e-02, -2.3397e-03,  3.7326e-03,  7.9592e-03,\n",
      "         4.4784e-03, -4.7033e-03, -8.2373e-03, -5.2692e-03, -2.1199e-03,\n",
      "         4.4711e-03, -1.6763e-03,  6.6929e-03,  5.8111e-03,  1.5262e-04,\n",
      "        -6.8341e-03, -8.7865e-03, -6.4996e-03, -5.6609e-03,  3.1490e-03,\n",
      "        -2.6856e-03, -1.5720e-03, -5.8609e-03,  2.6088e-03,  2.7732e-03,\n",
      "        -3.3193e-02,  3.0426e-03, -1.4652e-02,  6.4317e-03, -7.9249e-04,\n",
      "        -7.7355e-03,  2.3927e-03, -6.3232e-03, -3.4111e-03, -4.5153e-04,\n",
      "        -1.0123e-02, -6.6885e-03, -1.3338e-02,  1.6539e-03, -6.2623e-03,\n",
      "        -7.7340e-03,  6.2080e-03, -2.4150e-02, -2.5442e-04, -1.7355e-02,\n",
      "         2.8122e-03,  9.0486e-04, -8.0716e-03, -2.6040e-03, -4.0039e-03,\n",
      "        -5.0881e-03, -1.1187e-02,  1.5440e-03, -9.6337e-03, -5.3996e-03,\n",
      "         5.1064e-03, -1.1041e-02, -1.8721e-02, -1.6307e-02, -3.5815e-03,\n",
      "        -6.8381e-03, -1.0711e-02, -1.8824e-02,  2.4584e-03,  9.3913e-03,\n",
      "        -3.6912e-03,  3.4938e-03,  7.2786e-03,  2.7004e-03,  1.0162e-02,\n",
      "        -9.3015e-03, -5.4446e-04,  3.2322e-03,  4.4611e-03, -7.5626e-03,\n",
      "        -7.0143e-03, -5.4582e-04, -1.0640e-02, -1.2281e-02, -2.9098e-03,\n",
      "        -1.0855e-02, -4.2636e-03,  7.7370e-03, -1.1488e-02, -5.6070e-03,\n",
      "         5.6343e-03, -5.1835e-03, -3.2270e-02, -4.5921e-03, -3.7105e-03,\n",
      "        -1.3546e-03, -1.4626e-02,  3.4998e-03, -4.5562e-03,  9.4148e-03,\n",
      "         1.3484e-02, -9.9946e-03, -1.9585e-02, -5.1577e-03, -1.2491e-02,\n",
      "         2.1859e-03, -2.2348e-02, -7.5201e-03,  3.2404e-03, -9.1752e-03,\n",
      "        -7.3037e-03, -3.9787e-03,  3.5126e-03, -2.6108e-03, -4.1621e-03,\n",
      "        -3.6602e-03, -7.7069e-03, -1.2162e-02, -3.2834e-03, -5.3185e-04,\n",
      "        -1.8280e-03,  3.8120e-03, -4.7607e-03, -3.3326e-03, -3.3949e-03,\n",
      "        -2.0753e-02, -9.3708e-03, -1.1098e-02,  2.3046e-03, -2.8969e-03,\n",
      "         6.5295e-03,  3.4325e-03, -6.2699e-03, -1.0423e-03, -1.1592e-02,\n",
      "        -5.4376e-03, -2.8649e-03,  6.3376e-03, -2.8657e-03, -3.8697e-03,\n",
      "         2.0220e-03, -6.0879e-03, -1.2983e-02, -8.9899e-04,  1.0616e-03,\n",
      "        -4.0734e-03, -1.0295e-02,  4.9866e-03, -8.2509e-03, -7.5821e-03,\n",
      "         1.3395e-02, -2.8695e-03, -3.7554e-03, -4.5137e-03, -5.3716e-03,\n",
      "        -2.8421e-03,  1.9736e-03, -4.0295e-03, -5.2543e-03,  6.2240e-03,\n",
      "        -9.9807e-03, -6.9483e-03, -1.9620e-03,  1.2012e-02, -3.0358e-03,\n",
      "        -3.4865e-03, -3.4454e-03, -4.1626e-03, -9.1879e-03, -5.2967e-03,\n",
      "        -9.2156e-03, -1.8564e-03, -6.3721e-03, -1.5833e-03, -1.0569e-02,\n",
      "         1.3897e-03,  1.5692e-03, -6.3323e-03,  1.4016e-02, -5.0100e-03,\n",
      "         3.2997e-03,  7.5199e-03, -1.4192e-03, -9.8058e-03, -2.5864e-03,\n",
      "        -4.3976e-04, -7.4125e-03, -7.7334e-03, -1.3661e-03,  1.5643e-03,\n",
      "         1.1610e-03, -1.0470e-02,  1.3304e-03, -1.6284e-03, -2.3020e-03,\n",
      "         3.4112e-03, -9.9214e-03,  5.1406e-03, -7.1858e-04,  4.8884e-04,\n",
      "        -4.6322e-03, -2.9536e-03, -7.1510e-03, -1.3588e-02, -5.9386e-03,\n",
      "         8.7587e-04,  1.1068e-02, -9.1933e-04, -9.5824e-03,  7.1884e-04,\n",
      "        -3.6772e-03, -4.9137e-04, -1.8536e-03, -1.8510e-03,  1.5590e-02,\n",
      "        -4.5262e-04, -2.1405e-02,  1.6021e-04, -7.2021e-03, -2.6234e-03,\n",
      "        -1.5487e-02,  9.3956e-03, -4.6861e-04, -5.2934e-03, -4.0667e-03,\n",
      "        -8.1632e-03,  8.0823e-03, -4.5676e-03, -2.2451e-04, -3.9615e-03,\n",
      "        -4.8301e-03, -1.6254e-04, -5.6059e-03,  2.3433e-03, -4.6786e-03,\n",
      "        -6.0992e-03, -1.8110e-03, -8.9852e-03,  3.3727e-03,  1.9760e-03,\n",
      "        -1.3686e-02,  8.8306e-03, -7.3699e-03,  6.4889e-04, -1.1961e-02,\n",
      "        -2.6508e-03,  7.7728e-03, -5.2941e-03, -1.4669e-02, -5.3437e-03,\n",
      "         3.1800e-03,  4.9981e-03,  2.4365e-03, -5.9891e-03, -2.0427e-03,\n",
      "        -1.2077e-02, -7.8336e-04, -4.9021e-03, -5.5549e-03,  1.6265e-05,\n",
      "        -1.1395e-02, -1.2234e-03, -2.4032e-03,  4.4155e-03,  2.0397e-03,\n",
      "         7.1104e-05,  7.0831e-04,  1.0672e-02, -5.5994e-03, -9.7131e-04,\n",
      "        -1.0107e-03,  4.8357e-03, -4.3074e-03, -1.7975e-02, -1.1337e-05,\n",
      "        -2.2691e-04,  4.6167e-03,  5.4057e-03, -2.8362e-03, -7.9111e-03,\n",
      "        -6.2665e-03,  3.4201e-03, -2.8284e-03, -1.6473e-03, -1.3980e-02,\n",
      "        -7.8351e-03, -1.0474e-02, -1.2332e-03, -3.8176e-03, -3.0659e-03,\n",
      "        -7.0381e-03, -5.4154e-03,  9.6489e-03, -2.6236e-03,  6.6286e-04,\n",
      "        -5.6792e-03, -7.2180e-03, -2.9230e-03,  4.7721e-03, -3.4975e-03,\n",
      "        -6.1801e-03, -1.0005e-02,  1.3489e-02, -4.1000e-03, -1.0685e-02,\n",
      "         8.4414e-03,  1.5530e-04,  5.9993e-03, -3.4035e-03, -4.1950e-03,\n",
      "        -9.7684e-03, -1.1373e-02, -3.2011e-03, -3.5362e-03, -2.9271e-03,\n",
      "        -1.2166e-02,  5.5470e-04,  7.9459e-03,  1.8785e-04, -8.4601e-03,\n",
      "        -1.8675e-02,  6.0708e-03, -3.6344e-04,  2.9406e-03, -3.0872e-03,\n",
      "        -1.0357e-03, -2.3671e-03,  4.1830e-03,  1.0217e-03, -1.6107e-02,\n",
      "        -5.5663e-03,  4.1615e-03, -2.6230e-03, -2.2697e-03, -6.9065e-03,\n",
      "        -5.3276e-03,  7.2820e-03], device='cuda:0'), 'fc2.weight': tensor([[ 0.0319, -0.0237, -0.0437,  ...,  0.0823,  0.0047, -0.0442],\n",
      "        [ 0.0363,  0.0004, -0.0061,  ..., -0.0235,  0.0223, -0.0242],\n",
      "        [ 0.0583, -0.0179,  0.0128,  ...,  0.0245, -0.0199, -0.0402],\n",
      "        ...,\n",
      "        [ 0.0092, -0.0227,  0.0085,  ...,  0.0207,  0.0333,  0.0026],\n",
      "        [ 0.0486, -0.0013, -0.0379,  ..., -0.0169,  0.0105, -0.0197],\n",
      "        [ 0.0289,  0.0078, -0.0029,  ..., -0.0343,  0.0191,  0.0333]],\n",
      "       device='cuda:0'), 'fc2.bias': tensor([-0.0310, -0.0142, -0.0299, -0.0027,  0.0035, -0.0453, -0.0210, -0.0101,\n",
      "         0.0143,  0.0271], device='cuda:0')})\n"
     ]
    }
   ],
   "execution_count": 191
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-31T10:01:20.059810Z",
     "start_time": "2025-03-31T10:01:20.044547Z"
    }
   },
   "cell_type": "code",
   "source": [
    "secret_key = torch.load(\"secret_key.pth\")\n",
    "watermark_vector=torch.tensor([0., 1., 0., 1., 1., 0., 0., 1.], dtype=torch.float32)\n",
    "\n",
    "with torch.no_grad():\n",
    "      conv2_mean = model_prun.conv2.weight.mean(dim=0)  # Should be (1,3,3)\n",
    "      conv2_mean_flat = conv2_mean.view(-1)\n",
    "      extracted_watermark = torch.sigmoid(torch.matmul( secret_key .cpu(), conv2_mean_flat.cpu()))  # Fix dimensions\n",
    "      extracted_watermark_binary = (extracted_watermark > 0.5).float()\n",
    "\n",
    "      # print(type(extracted_watermark_binary))\n",
    "      # print(type(watermark_vector))\n",
    "\n",
    "      print(\"\\nOriginal Watermark:\", watermark_vector.cpu().numpy())\n",
    "      print(\"\\nExtracted Watermark:\", extracted_watermark_binary.cpu().numpy())\n",
    "      print(\"\\nBER\", compute_ber(watermark_vector.cpu(), extracted_watermark_binary.cpu()))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Watermark: [0. 1. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Extracted Watermark: [0. 1. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "BER 0.0\n"
     ]
    }
   ],
   "execution_count": 241
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "JyTNPP90MxsL",
    "ExecuteTime": {
     "end_time": "2025-03-31T10:01:22.193604Z",
     "start_time": "2025-03-31T10:01:21.964177Z"
    }
   },
   "source": [
    "\n",
    "\n",
    "def prune_smallest_percentage(model, percentage=0.1):\n",
    "    # Loop through model layers and prune based on smallest weights\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n",
    "            weight_tensor = module.weight.data\n",
    "\n",
    "            # Get the absolute value of weights and sort them\n",
    "            abs_weights = torch.abs(weight_tensor).view(-1)  # Flatten to 1D\n",
    "            sorted_weights, indices = torch.sort(abs_weights)  # Sort by absolute values\n",
    "            print('sorted_weights')\n",
    "            print(sorted_weights)\n",
    "\n",
    "            # Calculate the threshold for the smallest 'percentage' of weights\n",
    "            print(torch.max(sorted_weights))\n",
    "            num_weights_to_prune = int((len(sorted_weights) * percentage))\n",
    "            print('num_weights_to_prune')\n",
    "            print(num_weights_to_prune)\n",
    "            if num_weights_to_prune == len(sorted_weights):\n",
    "                threshold = sorted_weights[num_weights_to_prune-1]\n",
    "            else:\n",
    "                threshold = sorted_weights[num_weights_to_prune]\n",
    "            print(threshold)\n",
    "            print('threshold')\n",
    "\n",
    "            # Create a pruning mask: set weights smaller than the threshold to zero\n",
    "            mask = torch.abs(weight_tensor) < threshold\n",
    "\n",
    "            # Apply the mask to prune weights\n",
    "            weight_tensor1=weight_tensor\n",
    "            print(weight_tensor)\n",
    "            print('weight_tensor1')\n",
    "            weight_tensor[mask] = 0\n",
    "            print(weight_tensor)\n",
    "            print('weight_tensor')\n",
    "\n",
    "            print(\"\\nBER\", compute_ber(weight_tensor1.cpu(), weight_tensor.cpu()))\n",
    "\n",
    "\n",
    "            # Optionally, you can apply pruning to the layer (keeping track of mask)\n",
    "            # prune.custom_from_mask(module, 'weight', mask)\n",
    "            print(f\"Pruned {num_weights_to_prune} weights in layer {name}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "model_prun1= prune_smallest_percentage(model_copy, percentage=1.0)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sorted_weights\n",
      "tensor([0.0011, 0.0035, 0.0038, 0.0042, 0.0044, 0.0051, 0.0062, 0.0076, 0.0092,\n",
      "        0.0118, 0.0119, 0.0123, 0.0123, 0.0132, 0.0139, 0.0146, 0.0161, 0.0164,\n",
      "        0.0169, 0.0180, 0.0188, 0.0189, 0.0214, 0.0231, 0.0232, 0.0247, 0.0260,\n",
      "        0.0271, 0.0290, 0.0304, 0.0310, 0.0316, 0.0322, 0.0346, 0.0348, 0.0356,\n",
      "        0.0364, 0.0370, 0.0374, 0.0376, 0.0399, 0.0400, 0.0416, 0.0466, 0.0484,\n",
      "        0.0522, 0.0532, 0.0536, 0.0565, 0.0582, 0.0586, 0.0620, 0.0632, 0.0636,\n",
      "        0.0679, 0.0680, 0.0681, 0.0688, 0.0698, 0.0707, 0.0722, 0.0730, 0.0744,\n",
      "        0.0772, 0.0776, 0.0783, 0.0793, 0.0811, 0.0820, 0.0827, 0.0844, 0.0853,\n",
      "        0.0857, 0.0879, 0.0889, 0.0892, 0.0900, 0.0903, 0.0904, 0.0905, 0.0908,\n",
      "        0.0931, 0.0935, 0.0936, 0.0949, 0.0993, 0.1014, 0.1015, 0.1040, 0.1041,\n",
      "        0.1093, 0.1123, 0.1130, 0.1132, 0.1143, 0.1146, 0.1208, 0.1241, 0.1258,\n",
      "        0.1270, 0.1282, 0.1299, 0.1315, 0.1316, 0.1318, 0.1323, 0.1350, 0.1351,\n",
      "        0.1354, 0.1362, 0.1362, 0.1379, 0.1383, 0.1422, 0.1438, 0.1443, 0.1464,\n",
      "        0.1488, 0.1505, 0.1508, 0.1508, 0.1510, 0.1537, 0.1549, 0.1551, 0.1556,\n",
      "        0.1563, 0.1568, 0.1587, 0.1606, 0.1659, 0.1666, 0.1682, 0.1708, 0.1721,\n",
      "        0.1751, 0.1759, 0.1764, 0.1770, 0.1797, 0.1804, 0.1813, 0.1858, 0.1921,\n",
      "        0.1936, 0.1940, 0.1964, 0.1964, 0.1989, 0.2001, 0.2002, 0.2018, 0.2037,\n",
      "        0.2046, 0.2056, 0.2057, 0.2068, 0.2070, 0.2072, 0.2146, 0.2146, 0.2168,\n",
      "        0.2179, 0.2181, 0.2191, 0.2196, 0.2199, 0.2205, 0.2223, 0.2224, 0.2228,\n",
      "        0.2257, 0.2273, 0.2281, 0.2297, 0.2317, 0.2329, 0.2331, 0.2342, 0.2353,\n",
      "        0.2366, 0.2386, 0.2391, 0.2398, 0.2411, 0.2438, 0.2440, 0.2461, 0.2466,\n",
      "        0.2487, 0.2509, 0.2517, 0.2517, 0.2517, 0.2536, 0.2536, 0.2558, 0.2580,\n",
      "        0.2581, 0.2584, 0.2597, 0.2600, 0.2603, 0.2626, 0.2630, 0.2635, 0.2644,\n",
      "        0.2673, 0.2677, 0.2692, 0.2692, 0.2696, 0.2699, 0.2740, 0.2781, 0.2825,\n",
      "        0.2836, 0.2844, 0.2845, 0.2853, 0.2853, 0.2889, 0.2912, 0.2913, 0.2924,\n",
      "        0.2947, 0.2954, 0.2958, 0.2963, 0.2968, 0.2979, 0.2994, 0.3071, 0.3110,\n",
      "        0.3119, 0.3127, 0.3132, 0.3139, 0.3152, 0.3154, 0.3168, 0.3171, 0.3175,\n",
      "        0.3218, 0.3225, 0.3228, 0.3234, 0.3236, 0.3304, 0.3322, 0.3358, 0.3383,\n",
      "        0.3396, 0.3454, 0.3459, 0.3488, 0.3515, 0.3577, 0.3591, 0.3603, 0.3612,\n",
      "        0.3624, 0.3629, 0.3632, 0.3641, 0.3677, 0.3701, 0.3719, 0.3744, 0.3824,\n",
      "        0.3827, 0.3845, 0.3871, 0.3878, 0.3895, 0.3926, 0.3986, 0.4045, 0.4084,\n",
      "        0.4234, 0.4245, 0.4350, 0.4382, 0.4450, 0.4456, 0.4562, 0.4621, 0.4919],\n",
      "       device='cuda:0')\n",
      "tensor(0.4919, device='cuda:0')\n",
      "num_weights_to_prune\n",
      "288\n",
      "tensor(0.4919, device='cuda:0')\n",
      "threshold\n",
      "tensor([[[[ 0.0364, -0.0118,  0.0811],\n",
      "          [-0.1568,  0.3304, -0.0164],\n",
      "          [-0.1316, -0.2825, -0.0853]]],\n",
      "\n",
      "\n",
      "        [[[-0.1040, -0.2836,  0.2635],\n",
      "          [-0.3641,  0.2070,  0.3110],\n",
      "          [-0.0376,  0.2994, -0.1587]]],\n",
      "\n",
      "\n",
      "        [[[-0.3878,  0.0688,  0.2889],\n",
      "          [ 0.3228, -0.0161, -0.0536],\n",
      "          [ 0.0370,  0.0169, -0.2963]]],\n",
      "\n",
      "\n",
      "        [[[-0.1488, -0.0051, -0.2002],\n",
      "          [ 0.3119, -0.1132, -0.3218],\n",
      "          [-0.1379,  0.3127,  0.3454]]],\n",
      "\n",
      "\n",
      "        [[[-0.0908, -0.1422,  0.3827],\n",
      "          [-0.0132, -0.2228, -0.0827],\n",
      "          [-0.0620, -0.2853, -0.2536]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1208,  0.2487, -0.1315],\n",
      "          [-0.0936,  0.3515, -0.1770],\n",
      "          [-0.3719, -0.2072,  0.2853]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0119,  0.2366, -0.4234],\n",
      "          [ 0.3459,  0.1551, -0.3175],\n",
      "          [-0.2677,  0.3744,  0.0044]]],\n",
      "\n",
      "\n",
      "        [[[-0.3824, -0.0374,  0.3677],\n",
      "          [-0.4045, -0.0400,  0.3591],\n",
      "          [-0.3629,  0.1721,  0.3986]]],\n",
      "\n",
      "\n",
      "        [[[-0.2342,  0.2580,  0.0681],\n",
      "          [-0.1964,  0.2692, -0.2517],\n",
      "          [-0.2692, -0.2057,  0.2146]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1858, -0.1921,  0.0092],\n",
      "          [ 0.0346,  0.3168,  0.0484],\n",
      "          [ 0.0707, -0.3152,  0.0260]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0903, -0.3383,  0.2329],\n",
      "          [-0.1666,  0.0232,  0.1146],\n",
      "          [ 0.2196, -0.1556,  0.2386]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0993,  0.1464, -0.1041],\n",
      "          [ 0.3322,  0.2558, -0.2181],\n",
      "          [-0.1354, -0.0900, -0.3926]]],\n",
      "\n",
      "\n",
      "        [[[-0.4456, -0.1936, -0.2947],\n",
      "          [ 0.3234,  0.0879,  0.1764],\n",
      "          [ 0.1751,  0.1549,  0.2845]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0905, -0.1282,  0.1093],\n",
      "          [ 0.2699, -0.2968,  0.0783],\n",
      "          [-0.2630,  0.1383,  0.2068]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2600, -0.0348,  0.1563],\n",
      "          [-0.2581,  0.2297,  0.0304],\n",
      "          [-0.0744, -0.2046,  0.1989]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2223, -0.3577, -0.0188],\n",
      "          [ 0.3612,  0.0123, -0.2740],\n",
      "          [-0.2179,  0.3154,  0.0466]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2958, -0.2353, -0.2281],\n",
      "          [-0.0042,  0.0231,  0.1964],\n",
      "          [-0.1537,  0.0730,  0.2331]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1443,  0.0011,  0.0582],\n",
      "          [ 0.2146, -0.1123, -0.3603],\n",
      "          [ 0.0316,  0.2913,  0.0062]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1797, -0.2224, -0.2168],\n",
      "          [-0.1130,  0.0820,  0.1508],\n",
      "          [-0.2644,  0.0636, -0.0416]]],\n",
      "\n",
      "\n",
      "        [[[-0.3396, -0.0935,  0.2001],\n",
      "          [-0.0889, -0.4084,  0.3871],\n",
      "          [ 0.3632, -0.4621, -0.0035]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1318, -0.2924, -0.1143],\n",
      "          [ 0.1350, -0.2438,  0.2317],\n",
      "          [-0.0180,  0.1759, -0.0139]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3488,  0.2954,  0.0038],\n",
      "          [-0.1510, -0.0632, -0.2597],\n",
      "          [-0.3132, -0.1682,  0.2509]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2536, -0.0698,  0.0310],\n",
      "          [ 0.0565,  0.2781, -0.2461],\n",
      "          [-0.2440,  0.1708,  0.3071]]],\n",
      "\n",
      "\n",
      "        [[[-0.0076,  0.2398, -0.0892],\n",
      "          [-0.0522,  0.0247,  0.1362],\n",
      "          [ 0.0949, -0.2257,  0.2979]]],\n",
      "\n",
      "\n",
      "        [[[-0.4562, -0.3624, -0.2584],\n",
      "          [ 0.1270,  0.1438, -0.1940],\n",
      "          [ 0.0290,  0.2018,  0.3358]]],\n",
      "\n",
      "\n",
      "        [[[-0.1258,  0.2391,  0.1505],\n",
      "          [-0.0680, -0.1241,  0.2273],\n",
      "          [-0.0722,  0.0356,  0.1606]]],\n",
      "\n",
      "\n",
      "        [[[-0.0844, -0.0931,  0.2466],\n",
      "          [ 0.0679,  0.1015, -0.0586],\n",
      "          [ 0.1014, -0.3171,  0.0123]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2912,  0.0146,  0.3701],\n",
      "          [ 0.2037, -0.1362,  0.0776],\n",
      "          [-0.4350, -0.0793, -0.3139]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0189,  0.1659, -0.2673],\n",
      "          [ 0.2411,  0.0399, -0.4245],\n",
      "          [ 0.2603,  0.1323, -0.1299]]],\n",
      "\n",
      "\n",
      "        [[[-0.1508, -0.4382, -0.4919],\n",
      "          [ 0.2844,  0.2056, -0.2191],\n",
      "          [ 0.1804,  0.4450, -0.0271]]],\n",
      "\n",
      "\n",
      "        [[[-0.3225,  0.0772,  0.2205],\n",
      "          [ 0.0214,  0.2696, -0.2199],\n",
      "          [ 0.1351, -0.0532,  0.1813]]],\n",
      "\n",
      "\n",
      "        [[[-0.3236, -0.2626,  0.2517],\n",
      "          [ 0.3895,  0.2517,  0.0322],\n",
      "          [-0.3845, -0.0904,  0.0857]]]], device='cuda:0')\n",
      "weight_tensor1\n",
      "tensor([[[[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0000, -0.4919],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]]]], device='cuda:0')\n",
      "weight_tensor\n",
      "\n",
      "BER 0.0\n",
      "Pruned 288 weights in layer conv2\n",
      "sorted_weights\n",
      "tensor([2.5835e-10, 3.5645e-10, 4.4995e-10,  ..., 2.3455e-01, 2.4456e-01,\n",
      "        2.4663e-01], device='cuda:0')\n",
      "tensor(0.2466, device='cuda:0')\n",
      "num_weights_to_prune\n",
      "12845056\n",
      "tensor(0.2466, device='cuda:0')\n",
      "threshold\n",
      "tensor([[ 6.2921e-03,  6.5319e-04,  8.3345e-03,  ...,  1.6364e-03,\n",
      "          5.7225e-03,  6.4337e-03],\n",
      "        [-5.4560e-03, -1.1619e-02, -1.2772e-02,  ..., -8.5252e-04,\n",
      "          1.7323e-04, -2.5367e-03],\n",
      "        [ 6.4434e-04,  2.3930e-03, -2.9082e-04,  ...,  4.0597e-03,\n",
      "          2.9580e-03,  9.1356e-03],\n",
      "        ...,\n",
      "        [-3.5319e-03, -5.4476e-03, -6.2750e-03,  ...,  1.0250e-03,\n",
      "          1.4246e-03, -1.3331e-02],\n",
      "        [-1.0681e-03, -3.3718e-03, -1.2866e-02,  ...,  4.3470e-03,\n",
      "         -5.6703e-05, -9.8906e-03],\n",
      "        [ 4.7858e-03, -7.3827e-04,  3.9747e-03,  ..., -3.3904e-03,\n",
      "          4.0162e-03, -5.0322e-03]], device='cuda:0')\n",
      "weight_tensor1\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "weight_tensor\n",
      "\n",
      "BER 0.0\n",
      "Pruned 12845056 weights in layer fc1\n",
      "sorted_weights\n",
      "tensor([5.4165e-06, 1.2561e-05, 3.3629e-05,  ..., 1.3978e-01, 1.4825e-01,\n",
      "        1.8988e-01], device='cuda:0')\n",
      "tensor(0.1899, device='cuda:0')\n",
      "num_weights_to_prune\n",
      "5120\n",
      "tensor(0.1899, device='cuda:0')\n",
      "threshold\n",
      "tensor([[ 0.0319, -0.0237, -0.0437,  ...,  0.0823,  0.0047, -0.0442],\n",
      "        [ 0.0363,  0.0004, -0.0061,  ..., -0.0235,  0.0223, -0.0242],\n",
      "        [ 0.0583, -0.0179,  0.0128,  ...,  0.0245, -0.0199, -0.0402],\n",
      "        ...,\n",
      "        [ 0.0092, -0.0227,  0.0085,  ...,  0.0207,  0.0333,  0.0026],\n",
      "        [ 0.0486, -0.0013, -0.0379,  ..., -0.0169,  0.0105, -0.0197],\n",
      "        [ 0.0289,  0.0078, -0.0029,  ..., -0.0343,  0.0191,  0.0333]],\n",
      "       device='cuda:0')\n",
      "weight_tensor1\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "weight_tensor\n",
      "\n",
      "BER 0.0\n",
      "Pruned 5120 weights in layer fc2\n"
     ]
    }
   ],
   "execution_count": 242
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AvjhGtvlMy-e"
   },
   "source": [
    "# **base embeded--- model fine tune with same embeding**"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-31T10:01:24.904113Z",
     "start_time": "2025-03-31T10:01:24.894959Z"
    }
   },
   "cell_type": "code",
   "source": [
    "state_dict1 = model_prun.state_dict()\n",
    "state_dict2 = model_prun1.state_dict()\n",
    "\n",
    "for key in state_dict1:\n",
    "    if torch.equal(state_dict1[key], state_dict2[key]):\n",
    "        # print(state_dict1[key])\n",
    "        # print('*******************')\n",
    "        # print(state_dict2[key])\n",
    "        print(f\"Layer {key}: Weights are identical.\")\n",
    "    else:\n",
    "        print(f\"Layer {key}: Weights are different.\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer conv2.weight: Weights are different.\n",
      "Layer conv2.bias: Weights are identical.\n",
      "Layer fc1.weight: Weights are different.\n",
      "Layer fc1.bias: Weights are identical.\n",
      "Layer fc2.weight: Weights are different.\n",
      "Layer fc2.bias: Weights are identical.\n"
     ]
    }
   ],
   "execution_count": 243
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-31T10:01:27.290746Z",
     "start_time": "2025-03-31T10:01:27.276550Z"
    }
   },
   "cell_type": "code",
   "source": [
    "secret_key = torch.load(\"secret_key.pth\")\n",
    "watermark_vector=torch.tensor([0., 1., 0., 1., 1., 0., 0., 1.], dtype=torch.float32)\n",
    "\n",
    "with torch.no_grad():\n",
    "      conv2_mean = model_prun1.conv2.weight.mean(dim=0)  # Should be (1,3,3)\n",
    "      conv2_mean_flat = conv2_mean.view(-1)\n",
    "      extracted_watermark = torch.sigmoid(torch.matmul( secret_key .cpu(), conv2_mean_flat.cpu()))  # Fix dimensions\n",
    "      extracted_watermark_binary = (extracted_watermark > 0.5).float()\n",
    "\n",
    "      # print(type(extracted_watermark_binary))\n",
    "      # print(type(watermark_vector))\n",
    "\n",
    "      print(\"\\nOriginal Watermark:\", watermark_vector.cpu().numpy())\n",
    "      print(\"\\nExtracted Watermark:\", extracted_watermark_binary.cpu().numpy())\n",
    "      print(\"\\nBER\", compute_ber(watermark_vector.cpu(), extracted_watermark_binary.cpu()))\n",
    "\n",
    "#finish PRUNING"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Watermark: [0. 1. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "Extracted Watermark: [0. 1. 0. 0. 0. 1. 0. 1.]\n",
      "\n",
      "BER 0.375\n"
     ]
    }
   ],
   "execution_count": 244
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "874PmbC4CRbq"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
